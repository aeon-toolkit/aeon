{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599bd2b3-ed7b-42b4-b886-991e9a05688c",
   "metadata": {},
   "source": [
    "# Analysis of the speedups provided by similarity search module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57d08d-a74a-453a-a17c-4e359d5f88ee",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the gains in time and memory of the different methods we use in the similarity search module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4fd81-15e4-4139-a761-6ba7005d352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from aeon.similarity_search.subsequence import MASS, BruteForce\n",
    "from aeon.utils.numba.general import sliding_mean_std_one_series\n",
    "\n",
    "ggplot_styles = {\n",
    "    \"axes.edgecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"EBEBEB\",\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.grid.which\": \"both\",\n",
    "    \"axes.spines.left\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.bottom\": False,\n",
    "    \"grid.color\": \"white\",\n",
    "    \"grid.linewidth\": \"1.2\",\n",
    "    \"xtick.color\": \"555555\",\n",
    "    \"xtick.major.bottom\": True,\n",
    "    \"xtick.minor.bottom\": False,\n",
    "    \"ytick.color\": \"555555\",\n",
    "    \"ytick.major.left\": True,\n",
    "    \"ytick.minor.left\": False,\n",
    "}\n",
    "\n",
    "plt.rcParams.update(ggplot_styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f071d-0672-4242-9c8c-e8d4a62c7a4d",
   "metadata": {},
   "source": [
    "## Computing means and standard deviations for all subsequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ec2d8-e8e5-4ca6-8792-16cd93a2c705",
   "metadata": {},
   "source": [
    "When we want to compute a normalised distance, given a time series `X` of size `m` and a query `q` of size `l`, we have to compute the mean and standard deviation for all subsequences of size `l` in `X`. One could do this task by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b76314-ccf4-4c6d-96bc-0b1cb3c97f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_stride_trick(X, window):\n",
    "    \"\"\"\n",
    "    Use strides to generate rolling/sliding windows for a numpy array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray\n",
    "        numpy array\n",
    "    window : int\n",
    "        Size of the rolling window\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : numpy.ndarray\n",
    "        This will be a new view of the original input array.\n",
    "    \"\"\"\n",
    "    a = np.asarray(X)\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def get_means_stds(X, query_length):\n",
    "    \"\"\"Compute the means and standard deviations of rolling windows in a time series.\"\"\"\n",
    "    windows = rolling_window_stride_trick(X, query_length)\n",
    "    return windows.mean(axis=-1), windows.std(axis=-1)\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(12)\n",
    "size = 100\n",
    "query_length = 10\n",
    "\n",
    "# Create a random series with 1 feature and 'size' timesteps\n",
    "X = rng.random((1, size))\n",
    "means, stds = get_means_stds(X, query_length)\n",
    "print(means.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e7aa71-65e6-4975-bea0-2188368eed9a",
   "metadata": {},
   "source": [
    "One issue with this code is that it actually recompute a lot of information between the computation of mean and std of each windows. Suppose that the window we compute the mean for `W_i = {x_i, ..., x_{i+(l-1)}`, to do this, we sum all the elements and divide them by `l`. You then want to compute the mean for `W_{i+1} = {x_{i+1}, ..., x_{i+1+(l-1)}`, which shares most of its values with `W_i` expect for `x_i` and `x_{i+1+(l-1)`. \n",
    "\n",
    "The optimization here consists in keeping a rolling sum, we only compute the full sum of the `l` values for the first window `W_0`, then to obtain the sum for `W_1`, we remove `x_0` and add `x_{1+(l-1)}` from the sum of `W_0`. We can also a rolling squared sum to compute the standard deviation.\n",
    "\n",
    "The `sliding_mean_std_one_series` function implement the computation of the means and standard deviations using these two rolling sums. The last argument indicates the dilation to apply to the subsequence, which is not used here, hence the value of 1 in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48986d-dca0-44f2-9d7a-a67fc32731e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [500, 1000, 5000, 10000, 25000, 50000]\n",
    "query_lengths = [50, 100, 250, 500]\n",
    "times = pd.DataFrame(\n",
    "    index=pd.MultiIndex(levels=[[], []], codes=[[], []], names=[\"size\", \"query_length\"])\n",
    ")\n",
    "# A first run for numba compilations if needed\n",
    "sliding_mean_std_one_series(rng.random((1, 50)), 10, 1)\n",
    "for size in sizes:\n",
    "    for query_length in query_lengths:\n",
    "        X = rng.random((1, size))\n",
    "        _times = %timeit -r 3 -n 3 -q -o get_means_stds(X, query_length)\n",
    "        times.loc[(size, query_length), \"full computation\"] = _times.average\n",
    "        _times = %timeit -r 3 -n 3 -q -o sliding_mean_std_one_series(X, query_length, 1)\n",
    "        times.loc[(size, query_length), \"sliding_computation\"] = _times.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e14732-7675-463f-a4ed-2f903d12142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=len(query_lengths), figsize=(20, 5), dpi=200, sharey=True)\n",
    "for j, (i, grp) in enumerate(times.groupby(\"query_length\")):\n",
    "    grp.droplevel(1).plot(label=i, ax=ax[j])\n",
    "    ax[j].set_title(f\"query length {i}\")\n",
    "    ax[j].set_yscale(\"log\")\n",
    "ax[0].set_ylabel(\"time in seconds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f15e37-5735-4d33-b2cf-70235420b724",
   "metadata": {},
   "source": [
    "As you can see, the larger the size of `q`, the greater the speedups. This is because the larger the size of `q`, the more recomputation we avoid by using a sliding sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b7328-d104-44fa-98e3-365a7ee1629d",
   "metadata": {},
   "source": [
    "## Computing the Euclidean distance with a dot product obtained from convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e33cb7-c861-4aa4-b037-c341b9d85d05",
   "metadata": {},
   "source": [
    "The standard way to compute the (squared) euclidean distance between a query `Q = {q_1, ..., q_l}` and a candidate subsequence `W_i = {x_i, ..., x_{i+(l-1)}` is to compute it as $d(Q,W_i) = \\sum_j^l (x_{i+j} - q_j)^2$.\n",
    "\n",
    "We can also express this distance as $d(Q,W_i) = Q^2 + W_i^2 - 2Q.W_i$, in our case, we can use the fact a cross correlation can be used to compute $Q*X$ to obtain the dot products between $Q$ and all $W_i$. The timing difference become even more important for large input, when it becomes worth to use a fast Fourrier transform to compute the convolution in the frequency domain. See scipy.signal.convolve documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c34efd-2927-4a6f-9f3c-bc723e5229c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [500, 1000, 5000, 10000, 20000, 30000, 50000]\n",
    "query_lengths = [0.01, 0.05, 0.1, 0.2]\n",
    "times = pd.DataFrame(\n",
    "    index=pd.MultiIndex(levels=[[], []], codes=[[], []], names=[\"size\", \"query_length\"])\n",
    ")\n",
    "\n",
    "for size in sizes:\n",
    "    for _query_length in query_lengths:\n",
    "        query_length = int(_query_length * size)\n",
    "        X = rng.random((1, size))\n",
    "        q = rng.random((1, query_length))\n",
    "        mask = np.ones((1, size - query_length + 1), dtype=bool)\n",
    "        # Used for numba compilation before timings\n",
    "        mass = MASS(length=query_length).fit(X)\n",
    "        mass.compute_distance_profile(q)\n",
    "        dummy = BruteForce(length=query_length).fit(X)\n",
    "        dummy.compute_distance_profile(q)\n",
    "\n",
    "        _times = %timeit -r 3 -n 3 -q -o dummy.compute_distance_profile(q)\n",
    "        times.loc[(size, _query_length), \"Naive Euclidean distance\"] = _times.average\n",
    "\n",
    "        _times = %timeit -r 3 -n 3 -q -o mass.compute_distance_profile(q)\n",
    "        times.loc[(size, _query_length), \"Euclidean distance with MASS\"] = (\n",
    "            _times.average\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082b60c-b6ec-41a7-8566-2c6deca59860",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=len(query_lengths), figsize=(20, 5), dpi=200)\n",
    "for j, (i, grp) in enumerate(times.groupby(\"query_length\")):\n",
    "    grp.droplevel(1).plot(label=i, ax=ax[j])\n",
    "    ax[j].set_title(f\"query length {i}\")\n",
    "    ax[j].set_yscale(\"log\")\n",
    "ax[0].set_ylabel(\"time in seconds\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10127f4-6515-4ea5-a1d9-e14671eb70be",
   "metadata": {},
   "source": [
    "The same reasoning holds for the normalised (squared) euclidean distance, we can use the `normalize` parameter of the two estimators to set this option. In the normalised case, the formula used to computed the normalised (squared) euclidean distance is taken from the paper [Matrix Profile I: All Pairs Similarity Joins for Time Series](https://www.cs.ucr.edu/~eamonn/PID4481997_extend_Matrix%20Profile_I.pdf), see MASS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8dabbd-d11f-4aa9-a8f6-d489548ca852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [500, 1000, 5000, 10000, 20000, 30000, 50000]\n",
    "query_lengths = [0.01, 0.05, 0.1, 0.2]\n",
    "times = pd.DataFrame(\n",
    "    index=pd.MultiIndex(levels=[[], []], codes=[[], []], names=[\"size\", \"query_length\"])\n",
    ")\n",
    "\n",
    "for size in sizes:\n",
    "    for _query_length in query_lengths:\n",
    "        query_length = int(_query_length * size)\n",
    "        X = rng.random((1, size))\n",
    "        q = rng.random((1, query_length))\n",
    "        mask = np.ones((1, size - query_length + 1), dtype=bool)\n",
    "        # Used for numba compilation before timings\n",
    "        mass = MASS(length=query_length, normalize=True).fit(X)\n",
    "        mass.compute_distance_profile(q)\n",
    "        dummy = BruteForce(length=query_length, normalize=True).fit(X)\n",
    "        dummy.compute_distance_profile(q)\n",
    "\n",
    "        _times = %timeit -r 3 -n 3 -q -o dummy.compute_distance_profile(q)\n",
    "        times.loc[(size, _query_length), \"Naive Normalised Euclidean distance\"] = (\n",
    "            _times.average\n",
    "        )\n",
    "\n",
    "        _times = %timeit -r 3 -n 3 -q -o mass.compute_distance_profile(q)\n",
    "        times.loc[(size, _query_length), \"Normalised Euclidean distance with MASS\"] = (\n",
    "            _times.average\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701000b-9c17-45f7-a58c-64c22b88f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=len(query_lengths), figsize=(20, 5), dpi=200)\n",
    "for j, (i, grp) in enumerate(times.groupby(\"query_length\")):\n",
    "    grp.droplevel(1).plot(label=i, ax=ax[j])\n",
    "    ax[j].set_title(f\"query length {i}\")\n",
    "    ax[j].set_yscale(\"log\")\n",
    "ax[0].set_ylabel(\"time in seconds\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon_dev_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
