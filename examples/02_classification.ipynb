{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Time Series Classification with sktime\n",
    "\n",
    "Time Series Classification (TSC) involves training a model from a collection\n",
    " of time series (real valued, ordered, data) in order to predict a target variable.\n",
    " For example, we might want to build a model that can predict whether a patient\n",
    " is sick based on their ECG reading, or a persons type of movement based on\n",
    " the trace of the position of their hand. This notebook gives a quick guide to TSC get you started using\n",
    " the TSC specific classifiers in this toolkit. If you can use scikit-learn, it should\n",
    "  be easy.\n",
    "\n",
    "<img src=\"./img/tsc.png\" width=\"600\" alt=\"time series classification\"> [<i>&#x200B;</i>](./img/tsc.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Storage and Problem Types\n",
    "\n",
    "Time series can be univariate (each observation is a single value) or multivariate\n",
    "(each observation is a vector). For example, an ECG reading from a single\n",
    "sensor is a univariate\n",
    "series, but a motion trace of from a smart watch would be multivariate, with at\n",
    "least three dimensions (x,y,z co-ordinates). The image above is a univariate problem:\n",
    " each series has its own label. The dimension of the time series instance is also often called the\n",
    " channel. We recommend storing time series in 3D numpy array of shape (instance,\n",
    " dimension, time point) and where possible our single problem loaders will return a\n",
    " 3D numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting and data loading imports used in this notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sktime.datasets import load_arrow_head, load_basic_motions\n",
    "\n",
    "arrow, arrow_labels = load_arrow_head(split=\"train\")\n",
    "motions, motions_labels = load_basic_motions(split=\"train\")\n",
    "print(f\"ArrowHead series of type {type(arrow)} and shape {arrow.shape}\")\n",
    "print(f\"Motions type {type(motions)} of shape {motions_labels.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We tend to use 3D numpy even if the data is univariate, although all classifiers work\n",
    " with shape (instance, time point), currently some transformers do not work correctly\n",
    "  with 2D arrays. If your series are unequal length, have missing values or are\n",
    "  sampled at irregular time intervals, you should read the note book\n",
    "  datatypes_and_datasets (link).\n",
    "\n",
    "The UCR/UEA [TSC dataset archive](https://timeseriesclassification.com/) contains a\n",
    "large number of example TSC problems that have been used thousands of times in the\n",
    "literature to assess TSC algorithms. These datasets have certain characteristics that\n",
    "influence what data structure we use to store them in memory.\n",
    "\n",
    "Most datasets in the archive contain time series all the same length. For example,\n",
    "the [ArrowHead dataset](https://timeseriesclassification.com/description.php?Dataset=ArrowHead) we have just loaded consists of outlines of the images of arrow heads. The classification of projectile points is an important topic in anthropology.\n",
    "\n",
    "<img src=\"./img/arrow-heads.png\" width=\"600\" alt=\"arrow heads\">\n",
    "\n",
    "The shapes of the projectile points are converted into a sequence using the angle-based method as described in this [blog post](https://izbicki.me/blog/converting-images-into-time-series-for-data-mining.html) about converting images into time series for data mining.\n",
    "\n",
    "<img src=\"./img/from-shapes-to-time-series.png\" width=\"600\" alt=\"from shapes to time series\">\n",
    "\n",
    "Each instance consists of a single time series (i.e. the problem is univariate) of equal length and a class label based on shape distinctions such as the presence and location of a notch in the arrow. The data set consists of 210 instances, by default split into 36 train and 175 test instances.\n",
    "\n",
    "The [BasicMotions dataset](https://timeseriesclassification.com/description.php?Dataset=BasicMotions) is an example of a multivariate TSC problem. It was generated\n",
    " as part of a project where four students performed four activities whilst wearing a\n",
    " smartwatch. The watch collects 3D accelerometer and 3D gyroscope data. Each instance\n",
    "  involved a subject performing one of four tasks (walking, resting, running and\n",
    "  badminton) for ten seconds. Time series in this data set have six dimensions or\n",
    "  channels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(\n",
    "    f\"First and second dimensions of the first instance in BasicMotions data, \"\n",
    "    f\"(student {motions_labels[0]})\"\n",
    ")\n",
    "plt.plot(motions[0][0])\n",
    "plt.plot(motions[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(f\"First instance in ArrowHead data (class {arrow_labels[0]})\")\n",
    "plt.plot(arrow[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use a standard `sklearn` classifier for univariate, equal length\n",
    "classification problems, but it is unlikely to perform as well as bespoke time series\n",
    " classifiers, since `sklearn` classifiers ignore the sequence information in the variables.\n",
    "\n",
    "To apply `sklearn` classifiers directly, the data needs to be reshaped into a 2D\n",
    "numpy array. We also offer the ability to load univariate TSC problems directly in 2D\n",
    " arrays. Please note that currently, due to poor legacy design decisions, Transformers\n",
    "  do not work correctly  with 2D numpy classification problems, so we recommend using\n",
    "   3D numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "arrow2d = arrow.squeeze()\n",
    "arrow_test, arrow_test_labels = load_arrow_head(split=\"test\", return_type=\"numpy2d\")\n",
    "classifier.fit(arrow2d, arrow_labels)\n",
    "y_pred = classifier.predict(arrow_test)\n",
    "accuracy_score(arrow_test_labels, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Time Series Classifiers in scikit-time\n",
    "\n",
    "`sktime` contains the state of the art in time series classifiers in the package\n",
    "classification. These are grouped based on the data representation used to find\n",
    "discriminatory features. We provide a separate\n",
    "notebook for each of these (LINK HERE). An accurate and relatively\n",
    "fast classifier is the [ROCKET](https://link.springer.com/article/10.1007/s10618-020-00701-z) algorithm:\n",
    "\n",
    "We show the simplest use cases for classifiers and demonstrate how to build bespoke pipelines for time series classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sktime.classification.convolution_based import RocketClassifier\n",
    "\n",
    "rocket = RocketClassifier(num_kernels=2000)\n",
    "rocket.fit(arrow, arrow_labels)\n",
    "y_pred = rocket.predict(arrow_test)\n",
    "\n",
    "accuracy_score(arrow_test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Another accurate classifier for time series classification is version 2 of the\n",
    "[HIVE-COTE](https://link.springer.com/article/10.1007/s10994-021-06057-9) algorithm.\n",
    "(HC2).\n",
    "HC2 is relatively slow on small problems like these examples. However, it can be\n",
    "configured with an approximate maximum run time as follows (it may take a bit longer\n",
    "than 12 seconds to run this cell, very short times are approximate since there is a\n",
    "minimum amount of work the classifier needs to do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "\n",
    "hc2 = HIVECOTEV2(time_limit_in_minutes=0.2)\n",
    "hc2.fit(arrow, arrow_labels)\n",
    "y_pred = hc2.predict(arrow_test)\n",
    "\n",
    "accuracy_score(arrow_test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multivariate Classification\n",
    "To use `sklearn` classifiers directly on multivariate data, one option is to flatten\n",
    "the data so that (instances, dimensions, length) becomes (instances, dimensions*length)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "motions_test, motions_test_labels = load_basic_motions(split=\"test\")\n",
    "motions2d = motions.reshape(motions.shape[0], motions.shape[1] * motions.shape[2])\n",
    "motions2d_test = motions_test.reshape(\n",
    "    motions_test.shape[0], motions_test.shape[1] * motions_test.shape[2]\n",
    ")\n",
    "classifier.fit(motions2d, motions_labels)\n",
    "y_pred = classifier.predict(motions2d_test)\n",
    "accuracy_score(motions_test_labels, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, many scikit-learn classifiers, including ROCKET and HC2, are configured to\n",
    "work with multivariate input. This works exactly like univariate classification. For example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rocket = RocketClassifier(num_kernels=2000)\n",
    "rocket.fit(motions, motions_labels)\n",
    "y_pred = rocket.predict(motions_test)\n",
    "accuracy_score(motions_test_labels, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A list of classifiers capable of handling multivariate classification can be obtained\n",
    " with this code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sktime.registry import all_estimators\n",
    "\n",
    "# search for all classifiers that can handle multivariate time series. This will\n",
    "# give some UserWarnings if soft dependencies are not installed.\n",
    "all_estimators(\n",
    "    filter_tags={\"capability:multivariate\": True}, estimator_types=\"classifier\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "An alternative for MTSC is to build a univariate classifier on each dimension, then\n",
    "ensemble. Dimension ensembling can be easily done via `ColumnEnsembleClassifier`\n",
    "which fits classifiers independently to specified dimensions, then\n",
    "combines predictions through a voting scheme. The interface is\n",
    "similar to the `ColumnTransformer` from `sklearn`. The example below builds a DrCIF\n",
    "classifier on the first channel and a RocketClassifier on the fourth and fifth\n",
    "dimensions, ignoring the second, third and sixth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sktime.classification.compose import ColumnEnsembleClassifier\n",
    "from sktime.classification.interval_based import DrCIF\n",
    "\n",
    "col = ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"DrCIF0\", DrCIF(n_estimators=10, n_intervals=5), [0]),\n",
    "        (\"ROCKET3\", RocketClassifier(num_kernels=1000), [3, 4]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "col.fit(motions, motions_labels)\n",
    "y_pred = col.predict(motions_test)\n",
    "\n",
    "accuracy_score(motions_test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## sklearn Compatibility\n",
    "\n",
    "`sktime` classifiers are compatible with `sklearn` model selection and\n",
    "composition tools using `sktime` data formats. For example, cross-validation can\n",
    "be performed using the `sklearn` `cross_val_score` and `KFold` functionality:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "cross_val_score(rocket, arrow, y=arrow_labels, cv=KFold(n_splits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parameter tuning can be done using `sklearn` `GridSearchCV`. For example, we can tune\n",
    " the _k_ and distance measure for a K-NN classifier:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "knn = KNeighborsTimeSeriesClassifier()\n",
    "param_grid = {\"n_neighbors\": [1, 5], \"distance\": [\"euclidean\", \"dtw\"]}\n",
    "parameter_tuning_method = GridSearchCV(knn, param_grid, cv=KFold(n_splits=4))\n",
    "\n",
    "parameter_tuning_method.fit(arrow, arrow_labels)\n",
    "y_pred = parameter_tuning_method.predict(arrow_test)\n",
    "\n",
    "accuracy_score(arrow_test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Probability calibration is possible with the `sklearn` `CalibratedClassifierCV`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sktime.classification.interval_based import DrCIF\n",
    "\n",
    "calibrated_drcif = CalibratedClassifierCV(\n",
    "    base_estimator=DrCIF(n_estimators=10, n_intervals=5), cv=4\n",
    ")\n",
    "\n",
    "calibrated_drcif.fit(arrow, arrow_labels)\n",
    "y_pred = calibrated_drcif.predict(arrow_test)\n",
    "\n",
    "accuracy_score(arrow_test_labels, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "\n",
    "HIVECOTEV2(time_limit_in_minutes=0.2)\n",
    "hc2.fit(motions, motions_labels)\n",
    "y_pred = hc2.predict(motions_test)\n",
    "\n",
    "accuracy_score(motions_test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Classification with Unequal Length Series\n",
    "\n",
    "A common trait in time series data is that the series are of different length.\n",
    "We provide two such datasets: PLAID is a UTSC unequal length problem and\n",
    "JapaneseVowels is a MTSC dataset.  `numpy`\n",
    "does not support ragged arrays, so we lists of 2D numpy to store each instance\n",
    "(we assume that the channels for each time series are equal length). If you load the\n",
    "data from a file in .ts format (See HERE for more details), you do not need to worry\n",
    "about the data storage. There are two options to fit classifiers with unequal length\n",
    "series you can either use a classifier able to handle unequal length internally, or you transform the data so that it is equal\n",
    " length.\n",
    "\n",
    "At the time of writing the number of classifiers which natively support unequal length series is limited. The following outputs the current classifiers which support unequal length data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sktime.registry import all_estimators\n",
    "\n",
    "# search for all classifiers that can handle unequal length series. This will give\n",
    "# some UserWarnings if soft dependencies are not installed.\n",
    "all_estimators(\n",
    "    filter_tags={\"capability:unequal_length\": True}, estimator_types=\"classifier\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the classifier is capable of dealing with unequal length, then the use case is\n",
    "identical as to before. Alternatively, common approachs for unequal length series are\n",
    "to truncate the series to the length of the shortest or pad the series to the length\n",
    "of the longest. This can be done using tools in the transformations package."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sktime.datasets import load_plaid\n",
    "from sktime.transformations.panel.padder import PaddingTransformer\n",
    "from sktime.transformations.panel.truncation import TruncationTransformer\n",
    "\n",
    "X, y = load_plaid(split=\"train\")\n",
    "X2, y2 = load_plaid(split=\"test\")\n",
    "print(f\"Unequal length data stored in {type(X)} with shape = \", X.shape)\n",
    "trunc = TruncationTransformer()\n",
    "pad = PaddingTransformer()\n",
    "trunc.fit(X, y)\n",
    "truncX = trunc.transform(X)\n",
    "pad.fit(X, y)\n",
    "padX = trunc.transform(X)\n",
    "\n",
    "from sktime.datatypes import convert\n",
    "\n",
    "truncX = convert(truncX, from_type=\"nested_univ\", to_type=\"numpy3D\")\n",
    "padX = convert(padX, from_type=\"nested_univ\", to_type=\"numpy3D\")\n",
    "print(f\"Truncated PLAID data stored in {type(X)} with shape = \", X.shape)\n",
    "print(f\"Padded PLAID data stored in {type(X)} with shape = \", X.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X.iloc[0, 0])\n",
    "plt.show()\n",
    "plt.plot(truncX[0][0])\n",
    "plt.show()\n",
    "plt.plot(padX[0][0])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Background info and references for classifiers used here\n",
    "\n",
    "#### KNeighborsTimeSeriesClassifier\n",
    "\n",
    "One nearest neighbour (1-NN) classification with Dynamic Time Warping (DTW) is one of the oldest TSC approaches, and is commonly used as a performance benchmark.\n",
    "\n",
    "#### RocketClassifier\n",
    "The RocketClassifier is based on a pipeline combination of the ROCKET transformation (transformations.panel.rocket) and the sklearn RidgeClassifierCV classifier. The RocketClassifier is configurable to use variants MiniRocket and MultiRocket. ROCKET is based on generating random convolutional kernels. A large number are generated, then a linear classifier is built on the output.\n",
    "\n",
    "[1] Dempster, Angus, Fran√ßois Petitjean, and Geoffrey I. Webb. \"Rocket: exceptionally fast and accurate time series classification using random convolutional kernels.\" Data Mining and Knowledge Discovery (2020)\n",
    "[arXiv version](https://arxiv.org/abs/1910.13051)\n",
    "[DAMI 2020](https://link.springer.com/article/10.1007/s10618-020-00701-z)\n",
    "\n",
    "#### DrCIF\n",
    "The Diverse Representation Canonical Interval Forest Classifier (DrCIF) is an interval based classifier. The algorithm takes multiple randomised intervals from each series and extracts a range of features. These features are used to build a decision tree, which in turn are ensembled into a decision tree forest, in the style of a random forest.\n",
    "\n",
    "Original CIF classifier:\n",
    "[2] Matthew Middlehurst and James Large and Anthony Bagnall. \"The Canonical Interval Forest (CIF) Classifier for Time Series Classification.\" IEEE International Conference on Big Data (2020)\n",
    "[arXiv version](https://arxiv.org/abs/2008.09172)\n",
    "[IEEE BigData (2020)](https://ieeexplore.ieee.org/abstract/document/9378424?casa_token=8g_IG5MLJZ4AAAAA:ItxW0bY4eCRwfdV9kLvf-8a8X73UFCYUGU9D19PwrHigjivLJVchxHwkM3Btn7vvlOJ_0HiLRa3LCA)\n",
    "\n",
    "The DrCIF adjustment was proposed in [3].\n",
    "\n",
    "#### HIVE-COTE 2.0 (HC2)\n",
    "The HIerarchical VotE Collective of Transformation-based Ensembles is a meta ensemble that combines classifiers built on different representations. Version 2  combines DrCIF, TDE, an ensemble of RocketClassifiers called the Arsenal and the  ShapeletTransformClassifier. It is one of the most accurate classifiers on the UCR and UEA time series archives.\n",
    "\n",
    "[3] Middlehurst, Matthew, James Large, Michael Flynn, Jason Lines, Aaron Bostrom, and Anthony Bagnall. \"HIVE-COTE 2.0: a new meta ensemble for time series classification.\" Machine Learning (2021)\n",
    "[ML 2021](https://link.springer.com/article/10.1007/s10994-021-06057-9)\n",
    "\n",
    "#### Catch22\n",
    "\n",
    "The CAnonical Time-series CHaracteristics (Catch22) are a set of 22 informative and low redundancy features extracted from time series data. The features were filtered from 4791 features in the `hctsa` toolkit.\n",
    "\n",
    "[4] Lubba, Carl H., Sarab S. Sethi, Philip Knaute, Simon R. Schultz, Ben D. Fulcher, and Nick S. Jones. \"catch22: Canonical time-series characteristics.\" Data Mining and Knowledge Discovery (2019)\n",
    "[DAMI 2019](https://link.springer.com/article/10.1007/s10618-019-00647-x)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d800c14abb2bd109b7479fe8830174a66f0a4a77373f77c2c7334932e1a4922"
  },
  "kernelspec": {
   "name": "pycharm-9f7e3c09",
   "language": "python",
   "display_name": "PyCharm (sktime)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
