{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance based time series classification in aeon\n",
    "\n",
    "Distance based classifiers use a time series specific distance function to measure the\n",
    "similarity between time series. Time series distance functions are\n",
    "often called elastic distances, since they compensate for possible misalignment\n",
    "between series by shifting or editing the series.\n",
    "\n",
    "Dynamic time warping is the best known elastic distance measure. This image\n",
    "demonstrates how a warping path is found between two series\n",
    "<img src=\"./img/dtw.png\" width=\"400\" alt=\"A visualisation of dynamic time warping\">\n",
    "\n",
    "We have a range of elastic distance functions in the distances module. Please see the\n",
    " distances notebook for more information. Distance functions have been mostly used\n",
    " with a nearest neighbour (NN) classifier.\n",
    "\n",
    "<img src=\"./img/dtw2.png\" width=\"400\" alt=\"Example of warping two series to the best\n",
    "alignment.\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and list distance based classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-19T14:30:11.686582Z",
     "iopub.status.busy": "2020-12-19T14:30:11.686095Z",
     "iopub.status.idle": "2020-12-19T14:30:12.406787Z",
     "shell.execute_reply": "2020-12-19T14:30:12.407326Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "from sktime.datasets import load_italy_power_demand\n",
    "from sktime.registry import all_estimators\n",
    "\n",
    "X_train, y_train = load_italy_power_demand(split=\"train\", return_X_y=True)\n",
    "X_test, y_test = load_italy_power_demand(split=\"test\", return_X_y=True)\n",
    "X_test = X_test[:10]\n",
    "y_test = y_test[:10]\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distance based classifiers\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[('ElasticEnsemble',\n  sktime.classification.distance_based._elastic_ensemble.ElasticEnsemble),\n ('KNeighborsTimeSeriesClassifier',\n  sktime.classification.distance_based._time_series_neighbors.KNeighborsTimeSeriesClassifier),\n ('MatrixProfileClassifier',\n  sktime.classification.feature_based._matrix_profile_classifier.MatrixProfileClassifier),\n ('ShapeDTW', sktime.classification.distance_based._shape_dtw.ShapeDTW)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search for all classifiers that can handle multivariate time series. This will\n",
    "# give some UserWarnings if soft dependencies are not installed. Rerun to remove\n",
    "# warnings.\n",
    "all_estimators(\"classifier\", filter_tags={\"algorithm_type\": \"distance\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sktime.classification.distance_based import (\n",
    "    ElasticEnsemble,\n",
    "    KNeighborsTimeSeriesClassifier,\n",
    "    ShapeDTW,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-NN\n",
    "k-NN is often called a lazy classifier, because there is little work done in\n",
    "the fit operation. The fit operation simply stores the training data. When we want to\n",
    "make a prediction for a new time series, k-NN measures the distance between the new\n",
    "time series and all the series in the training data and records the class of the\n",
    "closest k train series. The class labels of these nearest neighbours are used to make\n",
    " a prediction: if they are all the same label, then that is the prediction. If they\n",
    " differ, then some form of voting mechanism is required. For example, we may predict\n",
    " the most common class label amongst the nearest neighbours for the test instance.\n",
    "\n",
    "KNeighborsTimeSeriesClassifier in aeon is configurable to use any of the distances\n",
    "functions in the distance module, or it can be passed a bespoke callable. You can set\n",
    " the number of neighbours and the weights. Weights are used in the prediction\n",
    " process when neightbours differ in class values. By default all neighbours have an\n",
    " equal vote. There is an option to weight by distance, meaning closer neighbours have\n",
    "  more weight in the vote."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "knn = KNeighborsTimeSeriesClassifier(distance=\"msm\", n_neighbors=3, weights=\"distance\")\n",
    "knn.fit(X_train, y_train)\n",
    "knn_preds = knn.predict(X_test)\n",
    "print(f\"3-NN Accuracy: {metrics.accuracy_score(y_test, knn_preds)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The first algorithm to significantly out perform 1-NN DTW on the UCR data was the\n",
    "Elastic Ensemble (EE) [1]. EE is a weighted ensemble of 11 1-NN classifiers with a\n",
    "range of elastic distance measures. It was the best performing distance based\n",
    "classifier in the bake off. Elastic distances can be slow, and EE requires cross\n",
    "validation to find the weights of each classifier in the ensemble. You can configure\n",
    "EE to use specified distance functions, and tell it how much\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "ee = ElasticEnsemble(\n",
    "    distance_measures=[\"dtw\", \"msm\"],\n",
    "    proportion_of_param_options=0.1,\n",
    "    proportion_train_in_param_finding=0.3,\n",
    "    proportion_train_for_test=0.5,\n",
    ")\n",
    "ee.fit(X_train, y_train)\n",
    "ee_preds = ee.predict(X_test)\n",
    "print(f\"EE Accuracy: {metrics.accuracy_score(y_test, ee_preds)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shape DTW\n",
    "Shape based DTW (ShapeDTW) \\citep{zhao18shape} works by extracting a set of shape descriptors (such as slope and derivative) over windows of each series. These series to series transformed data are then used with 1-NN with DTW.\n",
    "<img src=\"./img/dtw2.png\" width=\"400\" alt=\"Example of warping two series to the best\n",
    "alignment.\">\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shape = ShapeDTW()\n",
    "shape.fit(X_train, y_train)\n",
    "shape_preds = shape.predict(X_test)\n",
    "print(f\"EE Accuracy: {metrics.accuracy_score(y_test, shape_preds)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Proximity Forest\n",
    "\n",
    "Proximity Forest [3] is a distance based ensemble of decision trees. Its is the\n",
    "most accurate purely distance based technique for TSC that we know of. We do not\n",
    "currently have a working version of PF in aeon, but would very much like to have one.\n",
    "please see this issue. https://github.com/aeon-toolkit/aeon/issues/159"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing performance: coming soon\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "[1] Lines J, Bagnall A (2015) Time series classification with ensembles of elastic\n",
    "distance measures. Data Mining and Knowledge Discovery 29:565â€“592\n",
    "[2]\n",
    "[3]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
