{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Downloading and loading benchmarking datasets\n",
    "\n",
    "It is common to use standard collections of data to compare different estimators for\n",
    "classification, clustering, regression and forecasting. Some of these datasets are\n",
    "shipped with aeon in the datasets/data directory. However, the files are far too\n",
    "big to include them all. aeon p[rovides tools to download these data to use in benchmarking experiments.\n",
    "Classification and regression data are stored in .ts format. Forecasting\n",
    "data are stored in the equivalent .tsf format. See the [data formats notebook](examples/data_formats.ipynb) for more info.\n",
    "\n",
    "Classification and regression are loaded into 3D numpy arrays of shape `(n_cases, n_channels, n_timepoints)` if equal length\n",
    "or a list of `[n_cases]` of 2D numpy if `n_timepoints` is different for different\n",
    "cases. Forecasting data are loaded into pd.DataFrame. For more information on\n",
    "aeon data types see the [data storage notebook](examples/data_storage.ipynb).\n",
    "\n",
    "Note that this notebook is dependent on external websites, so will not function if\n",
    "you are not online or the associated website is down. We use the following three\n",
    "functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from aeon.datasets import load_classification, load_forecasting, load_regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time Series Classification Archive\n",
    "\n",
    "[UCR/TSML Time Series Classification Archive](https://timeseriesclassification.com)\n",
    "hosts the UCR univariate TSC archive [1], also available from [UCR](ucrweb) and\n",
    "the multivariate archive [2] (previously called the UEA archive, soon to change). We\n",
    "provide seven of these in the datasets/data directort: ACSF1, ArrowHead, BasicMotions,\n",
    "GunPoint, ItalyPowerDemand, JapaneseVowels and PLAID. The archive is much bigger. The\n",
    " last batch release was for 128 univariate [1] and 33 multivariate [2]. If you just\n",
    " want to download them all, please go to the [website]\n",
    " (https://timeseriesclassification.com)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariate length =  128\n",
      "Multivariate length =  33\n"
     ]
    }
   ],
   "source": [
    "from aeon.datasets.tsc_data_lists import multivariate, univariate\n",
    "\n",
    "# This file also contains sub lists by type, e.g. unequal length\n",
    "print(\"Univariate length = \", len(univariate))\n",
    "print(\"Multivariate length = \", len(multivariate))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A default train and test split is provided for this data. The file structure for a\n",
    "problem such as Chinatown is\n",
    "\n",
    "        <extract_path>/Chinatown/Chinatown_TRAIN.ts\n",
    "        <extract_path>/Chinatown/Chinatown_TEST.ts\n",
    "\n",
    "You can load these problems directly from TSC.com and load them into memory. Note by\n",
    "default, these functions return the data and associated metadata. This usage combines\n",
    " the train and test splits and loads them into one `X` and one `y` array."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X =  (363, 1, 24)\n",
      "First case =  [ 573.  375.  301.  212.   55.   34.   25.   33.  113.  143.  303.  615.\n",
      " 1226. 1281. 1221. 1081.  866. 1096. 1039.  975.  746.  581.  409.  182.]  has label =  1\n",
      "\n",
      "Meta data =  {'problemname': 'chinatown', 'timestamps': False, 'missing': False, 'univariate': True, 'equallength': True, 'classlabel': True, 'targetlabel': False, 'class_values': ['1', '2']}\n"
     ]
    }
   ],
   "source": [
    "X, y, meta = load_classification(\"Chinatown\")\n",
    "print(\"Shape of X = \", X.shape)\n",
    "print(\"First case = \", X[0][0], \" has label = \", y[0])\n",
    "print(\"\\nMeta data = \", meta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you look in aeon/datasets you should see a directory called `local_data`\n",
    "containing the Chinatown datasets. All of the zips have `.ts` files. Some also have\n",
    "`.arff` and `.txt` files. If you load again, it will not download again if the file is\n",
    "already there. If you want to store data somewhere else, you can specify a file path.\n",
    " Also, you can load the train and test separately. This code will download the data\n",
    " to Temp once, and load into separate train/test splits. The split argument is not\n",
    " case sensitive. Once downloaded, `load_classification` is a equivalent to a call to\n",
    " `load_from_tsfile`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Temp\\\\./Temp/BeetleFly\\\\BeetleFly_TRAIN.ts'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_train, y_train \u001B[38;5;241m=\u001B[39m \u001B[43mload_classification\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mBeetleFly\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextract_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./Temp/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTRAIN\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m      3\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m X_test, y_test \u001B[38;5;241m=\u001B[39m load_classification(\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBeetleFly\u001B[39m\u001B[38;5;124m\"\u001B[39m, extract_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./Temp/\u001B[39m\u001B[38;5;124m\"\u001B[39m, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m, return_metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain shape = \u001B[39m\u001B[38;5;124m\"\u001B[39m, X_train\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32mC:\\Code\\aeon\\aeon\\datasets\\_data_loaders.py:1282\u001B[0m, in \u001B[0;36mload_classification\u001B[1;34m(name, split, extract_path, return_metadata)\u001B[0m\n\u001B[0;32m   1233\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_classification\u001B[39m(name, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, extract_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, return_metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1234\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a classification dataset.\u001B[39;00m\n\u001B[0;32m   1235\u001B[0m \n\u001B[0;32m   1236\u001B[0m \u001B[38;5;124;03m    Loads a TSC dataset from extract_path, or from timeseriesclassification.com,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1280\u001B[0m \u001B[38;5;124;03m    >>> X, y, meta = load_classification(name=\"ArrowHead\") #DOCTEST +Skip\u001B[39;00m\n\u001B[0;32m   1281\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1282\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_tsc_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1283\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1284\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1285\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_X_y\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1286\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextract_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1287\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1288\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Code\\aeon\\aeon\\datasets\\_data_loaders.py:469\u001B[0m, in \u001B[0;36m_load_tsc_dataset\u001B[1;34m(name, split, return_X_y, return_type, extract_path, return_meta)\u001B[0m\n\u001B[0;32m    462\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m zipfile\u001B[38;5;241m.\u001B[39mBadZipFile \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    463\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    464\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid dataset name =\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not available on extract path =\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    465\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mextract_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Nor is it available on \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    466\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://timeseriesclassification.com/.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    467\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m--> 469\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_saved_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m    \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_X_y\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_X_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    474\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_module\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_module\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    475\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_dirname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_dirname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_meta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Code\\aeon\\aeon\\datasets\\_data_loaders.py:296\u001B[0m, in \u001B[0;36m_load_saved_dataset\u001B[1;34m(name, split, return_X_y, return_type, local_module, local_dirname, return_meta)\u001B[0m\n\u001B[0;32m    294\u001B[0m     fname \u001B[38;5;241m=\u001B[39m name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m split \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.ts\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    295\u001B[0m     abspath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(local_module, local_dirname, name, fname)\n\u001B[1;32m--> 296\u001B[0m     X, y, meta_data \u001B[38;5;241m=\u001B[39m \u001B[43mload_from_tsfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mabspath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_meta_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;66;03m# if split is None, load both train and test set\u001B[39;00m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m split \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\Code\\aeon\\aeon\\datasets\\_data_loaders.py:224\u001B[0m, in \u001B[0;36mload_from_tsfile\u001B[1;34m(full_file_path_and_name, replace_missing_vals_with, return_meta_data, return_type)\u001B[0m\n\u001B[0;32m    222\u001B[0m     full_file_path_and_name \u001B[38;5;241m=\u001B[39m full_file_path_and_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.ts\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;66;03m# Open file\u001B[39;00m\n\u001B[1;32m--> 224\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfull_file_path_and_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;66;03m# Read in headers\u001B[39;00m\n\u001B[0;32m    226\u001B[0m     meta_data \u001B[38;5;241m=\u001B[39m _load_header_info(file)\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;66;03m# load into list of numpy\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './Temp\\\\./Temp/BeetleFly\\\\BeetleFly_TRAIN.ts'"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_classification(\n",
    "    \"BeetleFly\", extract_path=\"./Temp/\", split=\"TRAIN\", return_metadata=False\n",
    ")\n",
    "X_test, y_test = load_classification(\n",
    "    \"BeetleFly\", extract_path=\"./Temp/\", split=\"test\", return_metadata=False\n",
    ")\n",
    "print(\"Train shape = \", X_train.shape)\n",
    "print(\"Test shape = \", X_test.shape)\n",
    "from aeon.datasets import load_from_tsfile\n",
    "\n",
    "X_train, y_train = load_from_tsfile(\n",
    "    full_file_path_and_name=\"./Temp/BeetleFly/BeetleFLY_TRAIN\"\n",
    ")\n",
    "print(\"Loaded directly shape = \", X_train.shape)\n",
    "\n",
    "X_test[0][0][:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time Series (Extrinsic) Regression\n",
    "\n",
    "[The Monash Time Series Extrinsic Regression Archive]() [3] repo (called extrinsic to\n",
    " diffentiate if from sliding window based regression) currently contains 19\n",
    " regression problems in .ts format. One of these, Covid3Month, is in `datasets\\data`.\n",
    "  The usage of `load_regression` is identical to `load_classification`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['AppliancesEnergy',\n 'AustraliaRainfall',\n 'BIDMCHR',\n 'BIDMCRR',\n 'BIDMCSpO2',\n 'BeijingPM10Quality',\n 'BeijingPM25Quality',\n 'BenzeneConcentration',\n 'Covid3Month',\n 'FloodModeling1',\n 'FloodModeling2',\n 'FloodModeling3',\n 'HouseholdPowerConsumption1',\n 'HouseholdPowerConsumption2',\n 'IEEEPPG',\n 'LiveFuelMoistureContent',\n 'NewsHeadlineSentiment',\n 'NewsTitleSentiment',\n 'PPGDalia']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aeon.datasets.dataset_collections import list_available_tser_datasets\n",
    "\n",
    "list_available_tser_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X =  (673, 1, 266)\n"
     ]
    }
   ],
   "source": [
    "X, y, meta = load_regression(\"FloodModeling1\")\n",
    "print(\"Shape of X = \", X.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time Series Forecasting\n",
    "\n",
    "The [Monash time series forecasting](https://forecastingdata.org/) repo contains a\n",
    "large number of forecasting data, including competition data such as M1, M3 and M4.\n",
    "Usage is the same as the other problems, although there is no provided train/test\n",
    "splits.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['australian_electricity_demand_dataset',\n 'car_parts_dataset_with_missing_values',\n 'car_parts_dataset_without_missing_values',\n 'cif_2016_dataset',\n 'covid_deaths_dataset',\n 'covid_mobility_dataset_with_missing_values',\n 'covid_mobility_dataset_without_missing_values',\n 'dominick_dataset',\n 'elecdemand_dataset',\n 'electricity_hourly_dataset',\n 'electricity_weekly_dataset',\n 'fred_md_dataset',\n 'hospital_dataset',\n 'kaggle_web_traffic_dataset_with_missing_values',\n 'kaggle_web_traffic_dataset_without_missing_values',\n 'kaggle_web_traffic_weekly_dataset',\n 'kdd_cup_2018_dataset_with_missing_values',\n 'kdd_cup_2018_dataset_without_missing_values',\n 'london_smart_meters_dataset_with_missing_values',\n 'london_smart_meters_dataset_without_missing_values',\n 'm1_monthly_dataset',\n 'm1_quarterly_dataset',\n 'm1_yearly_dataset',\n 'm3_monthly_dataset',\n 'm3_other_dataset',\n 'm3_quarterly_dataset',\n 'm3_yearly_dataset',\n 'm4_daily_dataset',\n 'm4_hourly_dataset',\n 'm4_monthly_dataset',\n 'm4_quarterly_dataset',\n 'm4_weekly_dataset',\n 'm4_yearly_dataset',\n 'nn5_daily_dataset_with_missing_values',\n 'nn5_daily_dataset_without_missing_values',\n 'nn5_weekly_dataset',\n 'pedestrian_counts_dataset',\n 'saugeenday_dataset',\n 'solar_10_minutes_dataset',\n 'solar_4_seconds_dataset',\n 'solar_weekly_dataset',\n 'sunspot_dataset_with_missing_values',\n 'sunspot_dataset_without_missing_values',\n 'tourism_monthly_dataset',\n 'tourism_quarterly_dataset',\n 'tourism_yearly_dataset',\n 'traffic_hourly_dataset',\n 'traffic_weekly_dataset',\n 'us_births_dataset',\n 'weather_dataset',\n 'wind_4_seconds_dataset',\n 'wind_farms_minutely_dataset_with_missing_values',\n 'wind_farms_minutely_dataset_without_missing_values']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aeon.datasets.dataset_collections import list_available_tsf_datasets\n",
    "\n",
    "list_available_tsf_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23000, 3)\n",
      "{'frequency': 'yearly', 'forecast_horizon': 6, 'contain_missing_values': False, 'contain_equal_length': False}\n",
      "  series_name     start_timestamp  \\\n",
      "0          T1 1979-01-01 12:00:00   \n",
      "1          T2 1979-01-01 12:00:00   \n",
      "2          T3 1979-01-01 12:00:00   \n",
      "3          T4 1979-01-01 12:00:00   \n",
      "4          T5 1979-01-01 12:00:00   \n",
      "\n",
      "                                        series_value  \n",
      "0  [5172.1, 5133.5, 5186.9, 5084.6, 5182.0, 5414....  \n",
      "1  [2070.0, 2104.0, 2394.0, 1651.0, 1492.0, 1348....  \n",
      "2  [2760.0, 2980.0, 3200.0, 3450.0, 3670.0, 3850....  \n",
      "3  [3380.0, 3670.0, 3960.0, 4190.0, 4440.0, 4700....  \n",
      "4  [1980.0, 2030.0, 2220.0, 2530.0, 2610.0, 2720....  \n"
     ]
    }
   ],
   "source": [
    "X, metadata = load_forecasting(\"m4_yearly_dataset\")\n",
    "print(X.shape)\n",
    "print(metadata)\n",
    "data = X.head()\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "[1] Dau et. al, The UCR time series archive, IEEE/CAA Journal of Automatica Sinica, 2019\n",
    "[2] Ruiz et. al, The great multivariate time series classification bake off: a review\n",
    " and experimental evaluation of recent algorithmic advances, Data Mining and\n",
    " Knowledge Discovery 35(2), 2021\n",
    "[3] Tan et. al, Time Series Extrinsic Regression, Data Mining and Knowledge\n",
    "Discovery, 2021\n",
    "[4] Godahewa et. al, Monash Time Series Forecasting Archive,Neural Information\n",
    "Processing Systems Track on Datasets and Benchmarks, 2021\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
