"""
autoplait.py - Python version of autoplait.c
Originally written by Yasuko Matsubara
Translated to Python in 2025

This is the main entry point for the AutoPlait algorithm.
"""

import math
import random
import sys
import warnings

import numpy as np
from sklearn.cluster import KMeans
from sklearn.exceptions import ConvergenceWarning
from aeon.segmentation.base import BaseSegmenter

warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# Numerical thresholds
EPSILON = 1e-6
ONE_MINUS_EPSILON = 0.999999
VERY_LARGE_COST = 1e11
MAX_NORMALISATION_VALUE = 1.0

# Segment signs
S0 = 1
S1 = -1

# Math
PI = math.pi
E = math.e
FLOATING_POINT_CONSTANT = 4 * 8
LOG_EPSILON = np.log(EPSILON)

# Output options
ENABLE_HMM_PRINT = True
OUTPUT_VITERBI_PATH = False

# ------------------------------
#     autoplait_header_py.py
# ------------------------------
"""
autoplait_header.py - Python version of autoplait.h
Originally written by Yasuko Matsubara
Translated to Python in 2025
"""

class AutoPlaitSegmenter(BaseSegmenter):
    """
    AutoPlait workspace structure.

    Attributes:
        x: Input sequence
        maxc: Maximum number of clusters
        maxseg: Maximum number of segments
        d: Dimension
        lmax: Maximum length
        cps: CutPointSearch structure
        baum: BAUM structure for EM algorithm
        q: Viterbi path
        vit, vit2: VITERBI structures
        x_tmp: Temporary input
        s: Current segment
        C: Candidate stack
        Opt: Optimal segments stack
        S: Segments stack
        costT: Total cost
        U: Uniform sampling
    """

    _tags = {
        "returns_dense": True,
        "fit_is_empty": False,
        "capability:multivariate": True
    }

    def __init__(self,
                 max_segments=250,
                 max_sequence_length=10000000,
                 min_k=1,
                 max_k=16,
                 num_samples=10,
                 max_baumn_segments=3,
                 delta=1,
                 max_iter=10,
                 max_k_means_iter=100,
                 min_infer_iter=3,
                 max_infer_iter=10,
                 default_variance=10,
                 max_variance=float('inf'),
                 min_variance=None,
                 segment_sample_ratio=0.03,
                 regime_sample_ratio=0.03,
                 samplimg_lm=0.1,
                 seed=None,
                 normalise=True,
                 verbose=False):

        self.max_segments = max_segments
        self.max_sequence_length = max_sequence_length
        self.min_k = min_k
        self.max_k = max_k
        self.num_samples = num_samples
        self.max_baumn_segments = max_baumn_segments
        self.delta = delta
        self.max_iter = max_iter
        self.max_k_means_iter = max_k_means_iter
        self.min_infer_iter = min_infer_iter
        self.max_infer_iter = max_infer_iter
        self.default_variance = default_variance
        self.max_variance = max_variance
        self.min_variance = EPSILON if min_variance is None else min_variance
        self.segment_sample_ratio = segment_sample_ratio
        self.regime_sample_ratio = regime_sample_ratio
        self.samplimg_lm = samplimg_lm

        self.x = None
        self.maxc = 0
        self.maxseg = 0
        self.d = 0
        self.lmax = max_sequence_length
        self.cps = CPS(self)
        self.baum = BAUM(self)
        self.q = None
        self.vit = VITERBI(self)
        self.vit2 = VITERBI(self)
        self.x_tmp = None
        self.s = None
        self.C = []
        self.Opt = []
        self.S = []
        self.costT = 0.0
        self.U = Regime(self)

        self.verbose = verbose
        self.axis = 0
        self.seed = seed
        self._do_normalise = normalise
        random.seed(seed)

        super().__init__(axis=0)

    def _set_up(self):
        """
            Allocate memory for AutoPlait structures.

            Parameters:
            pws -- AutoPlait workspace
            """

        self.maxc = int(math.log(self.lmax) + 20)
        self.maxseg = self.max_segments  # int(ws.lmax * 0.1)

        # Allocate Viterbi
        self.vit._set_up(self.maxseg, self.lmax, self.max_k)
        self.vit2._set_up(self.maxseg, self.lmax, self.max_k)
        self.q = np.zeros(self.lmax, dtype=np.int32)#ivector(0, ws.lmax)
        self.cps._set_up(self.max_k, self.lmax)

        # Allocate Baum
        n = self.maxseg
        if n > self.max_baumn_segments:
            n = self.max_baumn_segments

        self.x_tmp = [Input() for _ in range(n)]
        self.baum._set_up(n, self.lmax, self.max_k, self.d)

        # Allocate segbox
        self.s = [Regime(self) for _ in range(self.maxc)]
        for i in range(self.maxc):
            self.s[i]._set_up()

        for i in range(self.maxc):
            self.S.append(self.s[i])

        # For uniform sampling
        self.U._set_up()

    def _fit(self, X, y=None):
        if X.ndim == 1:
            X = X.reshape(-1, 1)
        self.d = X.shape[1]
        inp = Input()
        inp.m = min(X.shape[0], self.lmax)
        inp.id = 0
        inp.O = X
        self.x = inp
        self.lmax = self.x.m
        self._set_up()
        if self._do_normalise: self.normalise()

    def _predict(self, X):
        """
        Main entry point for AutoPlait algorithm.

        Parameters:
        pws -- AutoPlait workspace
        """

        if self.verbose: sys.stdout.write("---------\n")
        if self.verbose: sys.stdout.write("r|m|Cost \n")
        if self.verbose: sys.stdout.write("---------\n")

        self._plait()
        output = []
        for i in range(len(self.Opt)):
            regime = self.Opt[i]
            for j in range(regime.num_segments):
                output.append(((regime.segments[j]['start'] + regime.segments[j]['duration'] - 1), i))
        output = sorted(output, key=lambda x: x[0])
        cps = [x[0] for x in output][:-1]
        self.regimes = [x[1] for x in output]
        return np.array(cps)

    def get_regime(self, parent, label):
        """
        Get a segment box.

        Parameters:
        parent -- Parent label
        label -- Segment label

        Returns:
        Segment box
        """

        s = self.S.pop()
        if s is None:
            raise ValueError('Too small maxc')

        s.reset()
        s.model.reset(self.max_k, self.d)
        s.label = f"{parent}{label}"
        s.delta = 1.0 / float(self.lmax)

        return s

    def get_regimes(self):
        self._check_is_fitted()
        return self.regimes

    def _plait(self):
        """Main AutoPlait algorithm implementation."""
        # Initialize current_regime (X[0:m])
        current_regime = self.get_regime("", "")
        current_regime.costT = VERY_LARGE_COST
        current_regime.add_segment(0, self.x.m)
        current_regime.estimate_parameters()
        self.C.append(current_regime)

        while True:
            self.costT = self._MDLtotal()
            if not self.C:
                break
            current_regime = self.C.pop()

            # Create new segment sets
            regime0 = self.get_regime(current_regime.label, "0")
            regime1 = self.get_regime(current_regime.label, "1")

            # Try to split regime: current_regime->(regime0,regime1)
            self.cps._regimeSplit(current_regime, regime0, regime1)
            costT_regime01 = regime0.costT + regime1.costT

            # Split or not
            if costT_regime01 + current_regime.costT * self.regime_sample_ratio < current_regime.costT:
                self.C.append(regime0)
                self.C.append(regime1)
                self.S.append(current_regime)
            else:
                self.Opt.append(current_regime)
                self.S.append(regime0)
                self.S.append(regime1)

    def normalise(self):
        """Z-normalize sequences (mean=0, std=1)."""

        for d in range(self.d):
            mean = 0.0
            std = 0.0
            cnt = 0

            x = self.x
            cnt += x.m
            for j in range(x.m):
                mean += x.O[j][d]

            mean /= cnt

            # Compute std
            x = self.x
            for j in range(x.m):
                std += (x.O[j][d] - mean) ** 2

            std = math.sqrt(std / cnt)

            # Normalize
            x = self.x
            for j in range(x.m):
                x.O[j][d] = MAX_NORMALISATION_VALUE * (x.O[j][d] - mean) / std

    def _MDLtotal(self):
        """
        Calculate total MDL cost.

        Parameters:
        Opt -- Stack of optimal segments
        C -- Stack of candidate segments

        Returns:
        Total MDL cost
        """

        r = len(self.Opt) + len(self.C)
        m = sum(r.num_segments for r in self.Opt) + sum(r.num_segments for r in self.C)
        cost = sum(r.costT for r in self.Opt) + sum(r.costT for r in self.C)
        costT = cost + log_s(r) + log_s(m) + m * log_2(r) + (4 * 8) * r * r  # FLOATING_POINT_CONSTANT*r*r

        if self.verbose: sys.stdout.write(f"{r} {m} {costT:.0f} \n")

        return costT

class Input:
    """
    Input data class for HMM.

    Attributes:
        id: Sequence identifier
        tag: String tag for the sequence
        parent: Parent sequence id
        st: Start position in parent sequence
        O: Observation sequence [m][d]
        m: Length of sequence
        pid: Pattern id
    """

    def __init__(self):
        self.id = 0
        self.tag = ""
        self.parent = 0
        self.st = 0
        self.O = None
        self.m = 0
        self.pid = 0

class Regime:
    """
    A structure representing a segmented regime of a time series.

    Attributes:
        segments (np.ndarray): Structured NumPy array of segments, each with 'start' and 'duration'.
        nSb (int): Number of active segments.
        total_length (int): Total combined duration of all segments.
        costT (float): Total cost including model and encoding.
        costC (float): Cost of encoding the data.
        optimal (bool): Whether this regime is considered optimal.
        label (str): Label for the regime.
        model (HMM): Hidden Markov Model associated with this regime.
        delta (float): Proportion of time spent in this regime relative to total sequence length.
    """

    def __init__(self, ap_seg, label=''):

        self.ap_seg = ap_seg

        segment_dtype = np.dtype([('start', np.int32), ('duration', np.int32)])
        self.segments = np.zeros(self.ap_seg.max_segments, dtype=segment_dtype)
        self.num_segments = 0
        self.total_length = 0
        self.costT = 0.0
        self.costC = 0.0
        self.optimal = False
        self.label = label
        self.model = HMM(self.ap_seg.default_variance)
        self.delta = 0.0

    def _set_up(self):
        """
        Allocate memory for segment box.

        Parameters:
        s -- Segment box
        n -- Maximum number of sub-segments
        """

        self.num_segments = 0
        self.optimal = False
        self.model._set_up(self.ap_seg.max_k, self.ap_seg.d)

    def reset(self) -> None:
        """
        Resets the regime to an empty state, clearing all segments and resetting costs.
        """
        self.num_segments = 0
        self.total_length = 0
        self.costC = float('inf')
        self.costT = float('inf')

    def add_segment(self, start: int, duration: int) -> None:
        """
        Adds a segment to the regime and automatically removes any overlaps.

        Args:
            start (int): Start index of the new segment.
            duration (int): Duration of the new segment.

        Raises:
            RuntimeError: If the number of segments exceeds MAX_SEGMENTS.
        """
        if duration <= 0:
            return
        start = max(0, start)

        # Find insertion point
        loc = 0
        while loc < self.num_segments and self.segments[loc]['start'] <= start:
            loc += 1

        # Shift elements to make space
        if self.num_segments >= self.ap_seg.max_segments:
            raise RuntimeError("Too many segments")

        if loc < self.num_segments:
            self.segments[loc + 1:self.num_segments + 1] = self.segments[loc:self.num_segments]

        # Insert new segment
        self.segments[loc]['start'] = start
        self.segments[loc]['duration'] = duration
        self.num_segments += 1

        self._remove_overlap()
        self.total_length = self.segments[:self.num_segments]['duration'].sum()

    def _remove_overlap(self) -> None:
        """
        Merges overlapping or adjacent segments in-place.
        """
        i = 0
        while i < self.num_segments - 1:
            starts = self.segments[:self.num_segments]['start']
            durations = self.segments[:self.num_segments]['duration']
            ends = starts + durations

            seg0_start = starts[i]
            seg0_end = ends[i]
            seg1_start = starts[i + 1]
            seg1_end = ends[i + 1]

            if seg0_end + 1 >= seg1_start:
                # Merge seg0 and seg1
                self.segments[i]['duration'] = max(seg0_end, seg1_end) - seg0_start
                self.remove_segment(i + 1)
                # Don't advance i
            else:
                i += 1

    def remove_segment(self, index: int) -> None:
        """
        Removes a segment at the specified index from the regime.

        Args:
            index (int): Index of the segment to remove.

        Raises:
            IndexError: If index is out of bounds.
        """
        if index >= self.num_segments:
            raise IndexError(f"remove_segment: index {index} out of bounds (nSb={self.num_segments})")

        self.total_length -= self.segments[index]['duration']

        if index < self.num_segments - 1:
            self.segments[index:self.num_segments - 1] = self.segments[index + 1:self.num_segments]

        self.segments[self.num_segments - 1] = (0, 0)  # Optional clear
        self.num_segments -= 1

    def add_segment_with_overlap(self, start: int, duration: int) -> None:
        """
        Adds a segment to the regime without removing overlaps.

        Args:
            start (int): Start index of the segment.
            duration (int): Duration of the segment.

        Raises:
            RuntimeError: If the number of segments exceeds MAX_SEGMENTS.
        """
        if duration <= 0:
            return
        if self.num_segments >= self.ap_seg.max_segments:
            raise RuntimeError("Exceeded maximum number of segments")

        self.segments[self.num_segments]['start'] = start
        self.segments[self.num_segments]['duration'] = duration
        self.num_segments += 1
        self.total_length += duration

    def copy_to(self, target: 'Regime') -> None:
        """
        Copies all attributes and segment data to another Regime object.

        Args:
            target (Regime): The target Regime instance to copy into.
        """
        # Copy active segment data using slicing
        target.segments[:self.num_segments] = self.segments[:self.num_segments]

        target.num_segments = self.num_segments
        target.total_length = self.total_length
        target.costT = self.costT
        target.costC = self.costC
        target.delta = self.delta
        target.label = self.label

    def Split(self, X, st, len_val, x):
        """Split a sequence X into a sub-sequence x."""
        if st < 0:
            st = 0
        if X.m < st + len_val:
            len_val = X.m - st

        # In Python, we need to create a view of the array rather than pointer arithmetic
        x.O = X.O[st:st + len_val]
        x.m = len_val
        x.parent = X.id
        x.st = st
        x.tag = f"[{x.id}] {X.tag} [{st}-{st + len_val}]({len_val})"

    def max_segment_index(self) -> int:
        """
        Returns the index of the longest segment.

        Returns:
            int: Index of the segment with maximum duration, or -1 if no segments exist.
        """
        if self.num_segments == 0:
            return -1
        return int(np.argmax(self.segments[:self.num_segments]['duration']))

    def estimate_parameters(self) -> None:
        """
        Estimates an optimal HMM by testing values of k from MIN_K to MAX_K.
        Selects the model that minimizes the MDL.
        """
        self.costT = float('inf')
        optk = self.ap_seg.min_k

        for k in range(self.ap_seg.min_k, self.ap_seg.max_k + 1):
            previous_cost = self.costT
            self.estimate_hmm_k(k)
            self.compute_likelihood_mdl()

            if self.costT > previous_cost:
                optk = max(self.ap_seg.min_k, k - 1)
                break

        optk = min(self.ap_seg.max_k, max(self.ap_seg.min_k, optk))

        self.estimate_hmm_k(optk)
        self.compute_likelihood_mdl()

    def estimate_hmm_k(self, k: int) -> None:
        """
        Estimates the HMM parameters for a given number of states.

        Args:
            k (int): Number of HMM states to estimate.
        """
        k = min(self.ap_seg.max_k, max(self.ap_seg.min_k, k))  # Clamp k to [MIN_K, MAX_K]

        num_segments = min(self.num_segments, self.ap_seg.max_baumn_segments)
        for i in range(num_segments):
            self.Split(self.ap_seg.x, self.segments[i]['start'], self.segments[i]['duration'], self.ap_seg.x_tmp[i])

        self.model.k = k
        self.model.n = 0
        self.model.reset(k, self.ap_seg.d)

        self.ap_seg.baum.BaumWelch(self.model, num_segments, self.ap_seg.x_tmp, wantKMEANS=True)
        self.delta = self.num_segments / self.total_length if self.total_length else 0.0

    def mdl(self) -> float:
        """
        Calculates the Minimum Description Length (MDL) of the regime.

        Returns:
            float: The total MDL cost including encoding and model complexity.
        """
        k = self.model.k
        d = self.model.d
        m = self.total_length

        durations = self.segments[:self.num_segments]['duration']
        costLen = np.log2(durations).sum()
        costLen += m * np.log2(k)

        costM = FLOATING_POINT_CONSTANT * (k + k * k + 2 * k * d) + log_s(k)
        return self.costC + costLen + costM

    def compute_likelihood_mdl(self) -> None:
        """
        Computes the log-likelihood cost for all segments using the Viterbi algorithm,
        then computes the full MDL.
        """
        if self.num_segments == 0:
            self.costC = float('inf')
            self.costT = float('inf')
            return

        starts = self.segments[:self.num_segments]['start']
        durations = self.segments[:self.num_segments]['duration']

        costs = np.fromiter(
            (self.ap_seg.vit._viterbi(self.model, self.delta, int(st), int(dur))
             for st, dur in zip(starts, durations)),
            dtype=float,
            count=self.num_segments
        )
        self.costC = costs.sum()
        self.costT = self.mdl()

    def largest_segment(self) -> None:
        """
        Reduces the regime to its single largest segment.
        """
        index = self.max_segment_index()
        if index == -1:
            return

        self.reset()
        self.add_segment(
            self.segments[index]['start'],
            self.segments[index]['duration']
        )

class CPS:
    """
    CutPointSearch structure.

    Attributes:
        Pu, Pv, Pi, Pj: Probability arrays
        Su, Sv, Si, Sj: State arrays
        nSu, nSv, nSi, nSj: Counts
    """

    def __init__(self, ap_seg):
        self.ap_seg = ap_seg

        self.Pu = None
        self.Pv = None
        self.Pi = None
        self.Pj = None
        self.Su = None
        self.Sv = None
        self.Si = None
        self.Sj = None
        self.nSu = None
        self.nSv = None
        self.nSi = None
        self.nSj = None

    def _set_up(self, maxk, maxlen):
        """
        Allocate memory for Cut Point Search data structures.

        Parameters:
        cps -- CPS structure
        maxk -- Maximum number of states
        maxlen -- Maximum sequence length
        """
        self.Pu = np.zeros(maxk, dtype=np.float64)
        self.Pv = np.zeros(maxk, dtype=np.float64)
        self.Pi = np.zeros(maxk, dtype=np.float64)
        self.Pj = np.zeros(maxk, dtype=np.float64)
        self.Su = np.zeros((maxk, maxlen), dtype=np.int32)
        self.Sv = np.zeros((maxk, maxlen), dtype=np.int32)
        self.Si = np.zeros((maxk, maxlen), dtype=np.int32)
        self.Sj = np.zeros((maxk, maxlen), dtype=np.int32)
        self.nSu = np.zeros(maxk, dtype=np.int32)
        self.nSv = np.zeros(maxk, dtype=np.int32)
        self.nSi = np.zeros(maxk, dtype=np.int32)
        self.nSj = np.zeros(maxk, dtype=np.int32)

    def _search_aux(self, st, length, regime0, regime1):
        """
        Auxiliary function for Cut Point Search.

        Parameters:
            st (int): Start index of the segment
            length (int): Length of the segment
            regime0 (Regime): First regime model
            regime1 (Regime): Second regime model

        Returns:
            float: Coding cost
        """

        # Extract HMMs and deltas
        m0, d0 = regime0.model, regime0.delta
        m1, d1 = regime1.model, regime1.delta
        k0, k1 = m0.k, m1.k
        O = self.ap_seg.x.O
        O_window = O[st:st + length]

        # Handle degenerate case early
        if d0 <= 0 or d1 <= 0:
            raise ValueError('Degenerate dlta <= 0')

        # Alias CPS structures
        Pu, Pv = self.Pu, self.Pv
        Pi, Pj = self.Pi, self.Pj
        Su, Sv = self.Su, self.Sv
        Si, Sj = self.Si, self.Sj
        nSu, nSv = self.nSu, self.nSv
        nSi, nSj = self.nSi, self.nSj

        # Reset path state
        nSu[:k0] = 0
        nSv[:k0] = 0
        nSi[:k1] = 0
        nSj[:k1] = 0

        # Precompute emissions
        log_emission0 = batch_log_pdf(m0, k0, length, O_window)  # shape: (k0, length)
        log_emission1 = batch_log_pdf(m1, k1, length, O_window)  # shape: (k1, length)

        # Precompute log(pi) and log(A) values
        log_pi0 = np.log(m0.pi + EPSILON)
        log_pi1 = np.log(m1.pi + EPSILON)
        log_A0 = np.log(m0.A + EPSILON)
        log_A1 = np.log(m1.A + EPSILON)

        # Precompute transition constants
        log_d0 = math.log(d0)
        log_d1 = math.log(d1)
        log_1md0 = math.log(1.0 - d0)
        log_1md1 = math.log(1.0 - d1)

        # Initialize log-probabilities at t = 0
        Pv[:k0] = log_d1 + log_pi0[:k0] + log_emission0[:, 0]
        Pj[:k1] = log_d0 + log_pi1[:k1] + log_emission1[:, 0]

        # Dynamic programming: for t >= 1
        for t in range(st + 1, st + length):
            offset = t - st

            # --- Update Pu[t] ---

            # Find best path from previous Pj
            maxj = self._findMax(Pj, k1)

            for u in range(k0):
                log_emit = log_emission0[u, offset]

                # Case: switch to m0
                switch_score = Pj[maxj] + log_d1 + log_pi0[u] + log_emit

                # Case: stay in m0 (vectorized over v)
                vals = log_1md0 + Pv[:k0] + log_A0[:k0, u] + log_emit
                maxv = int(np.argmax(vals))
                stay_score = vals[maxv]

                # Choose better path
                if switch_score > stay_score:
                    Pu[u] = switch_score
                    nSu[u] = self._copy_path(Sj[maxj], nSj[maxj], Su[u])
                    Su[u][nSu[u]] = t
                    nSu[u] += 1
                else:
                    Pu[u] = stay_score
                    nSu[u] = self._copy_path(Sv[maxv], nSv[maxv], Su[u])

            # --- Update Pi[t] ---

            maxv = self._findMax(Pv, k0)

            for i in range(k1):
                log_emit = log_emission1[i, offset]

                # Case: switch to m1
                switch_score = Pv[maxv] + log_d0 + log_pi1[i] + log_emit

                # Case: stay in m1 (vectorized over j)
                vals = log_1md1 + Pj[:k1] + log_A1[:k1, i] + log_emit
                maxj = int(np.argmax(vals))
                stay_score = vals[maxj]

                if switch_score > stay_score:
                    Pi[i] = switch_score
                    nSi[i] = self._copy_path(Sv[maxv], nSv[maxv], Si[i])
                    Si[i][nSi[i]] = t
                    nSi[i] += 1
                else:
                    Pi[i] = stay_score
                    nSi[i] = self._copy_path(Sj[maxj], nSj[maxj], Si[i])

            # --- Swap buffers for next timestep ---
            Pu, Pv = Pv, Pu
            Pi, Pj = Pj, Pi
            Su, Sv = Sv, Su
            Si, Sj = Sj, Si
            nSu, nSv = nSv, nSu
            nSi, nSj = nSj, nSi

        # Determine which path has higher likelihood
        maxv = int(np.argmax(Pv[:k0]))
        maxj = int(np.argmax(Pj[:k1]))
        score_v = Pv[maxv]
        score_j = Pj[maxj]

        if score_v > score_j:
            path, npath, lh = Sv[maxv], nSv[maxv], score_v
            firstID = (-1) ** npath * S0
        else:
            path, npath, lh = Sj[maxj], nSj[maxj], score_j
            firstID = (-1) ** npath * S1

        # Segment reconstruction
        curSt = st
        flip = firstID  # Either S0 or S1 depending on best regime

        for i in range(npath):
            nxtSt = path[i]
            target_regime = regime0 if flip == S0 else regime1
            target_regime.add_segment(curSt, nxtSt - curSt)
            curSt = nxtSt
            flip *= -1  # Alternate regime

        # Final segment
        target_regime = regime0 if flip == S0 else regime1
        target_regime.add_segment(curSt, st + length - curSt)

        # Compute and return coding cost in bits
        costC = -lh / math.log(2.0)
        return costC

    def CPSearch(self, current_regime, regime0, regime1):
        """
        Cut Point Search algorithm.

        Parameters:
        current_regime -- Input segment box
        regime0 -- First output segment box
        regime1 -- Second output segment box
        ap_seg -- AutoPlait workspace

        Returns:
        Likelihood (coding cost)
        """

        regime0.reset()
        regime1.reset()

        lh = 0
        for i in range(current_regime.num_segments):
            lh += self._search_aux(current_regime.segments[i]['start'], current_regime.segments[i]['duration'], regime0,
                              regime1)

        return lh

    def _cps(self, current_regime, regime0, regime1, RM):
        """
        Cut point search with optional noise removal.

        Parameters:
        current_regime -- Source segment
        regime0 -- First segment
        regime1 -- Second segment
        RM -- Flag for noise removal
        """
        self.CPSearch(current_regime, regime0, regime1)

        if RM:
            self._removeNoise(current_regime, regime0, regime1)

        regime0.compute_likelihood_mdl()
        regime1.compute_likelihood_mdl()

    def _regimeEst_aux(self, current_regime, regime0, regime1):
        """
        Estimate regimes from segments.

        Parameters:
        current_regime -- Source segment
        regime0 -- First output segment
        regime1 -- Second output segment
        """
        opt0 = self.ap_seg.get_regime("", "")
        opt1 = self.ap_seg.get_regime("", "")

        for i in range(self.ap_seg.max_infer_iter):
            # Phase 1: Estimate parameters
            regime0.largest_segment()
            regime1.largest_segment()
            regime0.estimate_parameters()
            regime1.estimate_parameters()

            # Phase 2: Find cut-points
            self._cps(current_regime, regime0, regime1, True)

            if regime0.num_segments == 0 or regime1.num_segments == 0:
                break  # Avoid null inference

            # If improving, update the optimal segment set
            diff = (opt0.costT + opt1.costT) - (regime0.costT + regime1.costT)
            if diff > 0:
                regime0.copy_to(opt0)
                regime1.copy_to(opt1)
            # If not improving, then break iteration (efficient convergence)
            elif i >= self.ap_seg.min_infer_iter:
                break

        opt0.copy_to(regime0)
        opt1.copy_to(regime1)
        self.ap_seg.S.append(opt0)
        self.ap_seg.S.append(opt1)

    def _regimeSplit(self, current_regime, regime0, regime1):
        """
        Split a regime into two.

        Parameters:
        current_regime -- Source segment
        regime0 -- First output segment
        regime1 -- Second output segment
        ap_seg -- AutoPlaitSegmenter
        """
        seedlen = int(math.ceil(self.ap_seg.lmax * self.ap_seg.samplimg_lm))
        # Initialize HMM parameters
        self._findCentroid(current_regime, regime0, regime1, self.ap_seg.num_samples, seedlen)
        self._regimeEst_aux(current_regime, regime0, regime1)

        if regime0.num_segments == 0 or regime1.num_segments == 0:
            return

        # Final model estimation
        regime0.estimate_parameters()
        regime1.estimate_parameters()

    def _removeNoise(self, current_regime, regime0, regime1):
        """
        Remove noise from segments.

        Parameters:
        current_regime -- Source segment
        regime0 -- First segment
        regime1 -- Second segment
        """
        if regime0.num_segments <= 1 and regime1.num_segments <= 1:
            return

        # Default pruning
        per = self.ap_seg.segment_sample_ratio
        self._removeNoise_aux(regime0, regime1, per)
        costC = self._scanMinDiff(current_regime, regime0, regime1)

        # Optimal segment set
        opt0 = self.ap_seg.get_regime("", "")
        opt1 = self.ap_seg.get_regime("", "")
        regime0.copy_to(opt0)
        regime1.copy_to(opt1)
        prev = VERY_LARGE_COST

        # Find optimal pruning point
        while per <= self.ap_seg.segment_sample_ratio * 10:
            if costC >= VERY_LARGE_COST:
                break

            per *= 2
            self._removeNoise_aux(regime0, regime1, per)

            if regime0.num_segments <= 1 or regime1.num_segments <= 1:
                break

            costC = self._scanMinDiff(current_regime, regime0, regime1)

            if prev > costC:
                regime0.copy_to(opt0)
                regime1.copy_to(opt1)
            else:
                break

            prev = costC

        opt0.copy_to(regime0)
        opt1.copy_to(regime1)
        self.ap_seg.S.append(opt0)
        self.ap_seg.S.append(opt1)

    def _findCentroid(self, current_regime, regime0, regime1, nsamples, seedlen):
        """
        Find optimal centroids for segmentation.

        Parameters:
        current_regime -- Source segment
        regime0 -- First output segment
        regime1 -- Second output segment
        nsamples -- Number of samples
        seedlen -- Seed length

        Returns:
        Minimum cost
        """
        costMin = VERY_LARGE_COST

        # Keep best seeds
        regime0stB, regime1stB, regime0lenB, regime1lenB = 0, 0, 0, 0  # Best

        # Make sample set
        self.UniformSet(current_regime, seedlen, nsamples, self.ap_seg.U)

        # Start uniform sampling
        for iter1 in range(self.ap_seg.U.num_segments):
            for iter2 in range(iter1 + 1, self.ap_seg.U.num_segments):
                self.UniformSampling(regime0, regime1, seedlen, iter1, iter2, self.ap_seg.U)

                if regime0.num_segments == 0 or regime1.num_segments == 0:
                    continue  # Not sufficient

                # Copy positions
                regime0stC = regime0.segments[0]['start']
                regime0lenC = regime0.segments[0]['duration']
                regime1stC = regime1.segments[0]['start']
                regime1lenC = regime1.segments[0]['duration']

                # Estimate HMM
                regime0.estimate_hmm_k(self.ap_seg.min_k)
                regime1.estimate_hmm_k(self.ap_seg.min_k)

                # Cut point search
                self._cps(current_regime, regime0, regime1, True)

                if regime0.num_segments == 0 or regime1.num_segments == 0:
                    continue

                if costMin > regime0.costT + regime1.costT:
                    # Update best seeds
                    costMin = regime0.costT + regime1.costT
                    regime0stB = regime0stC
                    regime0lenB = regime0lenC
                    regime1stB = regime1stC
                    regime1lenB = regime1lenC

        if costMin == VERY_LARGE_COST:
            self.FixedSampling(current_regime, regime0, regime1, seedlen)
            return VERY_LARGE_COST

        regime0.reset()
        regime1.reset()
        regime0.add_segment(regime0stB, regime0lenB)
        regime1.add_segment(regime1stB, regime1lenB)

        return costMin

    def _scanMinDiff(self, current_regime, regime0, regime1):
        """
        Scan for minimum difference.

        Parameters:
        current_regime -- Source segment
        regime0 -- First segment
        regime1 -- Second segment

        Returns:
        Minimum difference cost
        """
        diff = [0.0]  # Use list to simulate pass-by-reference
        loc0 = self._findMinDiff(regime0, regime1, diff)
        loc1 = self._findMinDiff(regime1, regime0, diff)

        if loc0 == -1 or loc1 == -1:
            return VERY_LARGE_COST

        tmp0 = self.ap_seg.get_regime("", "")
        tmp1 = self.ap_seg.get_regime("", "")
        tmp0.add_segment(regime0.segments[loc0]['start'], regime0.segments[loc0]['duration'])
        tmp1.add_segment(regime1.segments[loc1]['start'], regime1.segments[loc1]['duration'])
        tmp0.estimate_hmm_k(self.ap_seg.min_k)
        tmp1.estimate_hmm_k(self.ap_seg.min_k)
        costC = self.CPSearch(current_regime, tmp0, tmp1)
        self.ap_seg.S.append(tmp0)
        self.ap_seg.S.append(tmp1)

        return costC

    def _findMinDiff(self, regime0, regime1, diffp):
        """
        Find minimum difference between segments.

        Parameters:
        regime0 -- First segment
        regime1 -- Second segment
        diffp -- Reference to store difference value

        Returns:
        Location of minimum difference
        """
        min_val = VERY_LARGE_COST
        loc = -1

        for i in range(regime0.num_segments):
            st = regime0.segments[i]['start']
            length = regime0.segments[i]['duration']
            costC0 = self.ap_seg.vit._viterbi(regime0.model, regime0.delta, st, length)
            costC1 = self.ap_seg.vit._viterbi(regime1.model, regime1.delta, st, length)
            diff = costC1 - costC0

            if min_val > diff:
                loc = i
                min_val = diff

        diffp[0] = min_val
        return loc


    def _removeNoise_aux(self, regime0, regime1, per):
        """
        Auxiliary function for noise removal.

        Parameters:
        current_regime -- Source segment
        regime0 -- First segment
        regime1 -- Second segment
        per -- Percentage threshold
        """
        if per == 0:
            return

        mprev = VERY_LARGE_COST
        th = self.ap_seg.costT * per

        while mprev > regime0.num_segments + regime1.num_segments:
            mprev = regime0.num_segments + regime1.num_segments

            # Find minimum segment
            diff0 = [0.0]  # Use list to simulate pass-by-reference
            diff1 = [0.0]
            loc0 = self._findMinDiff(regime0, regime1, diff0)
            loc1 = self._findMinDiff(regime1, regime0, diff1)

            if diff0[0] < diff1[0]:
                min_val = diff0[0]
                id_min = 0
            else:
                min_val = diff1[0]
                id_min = 1

            # Check remove or not
            if min_val < th:
                if id_min == 0:
                    regime1.add_segment(regime0.segments[loc0]['start'], regime0.segments[loc0]['duration'])
                    regime0.remove_segment(loc0)
                else:
                    regime0.add_segment(regime1.segments[loc1]['start'], regime1.segments[loc1]['duration'])
                    regime1.remove_segment(loc1)

    def _findMax(self, P, k):
        """
        Find the index of the maximum value in array P.

        Parameters:
        P -- Array of values
        k -- Length of array

        Returns:
        Index of maximum value
        """
        loc = -1
        max_val = float('-inf')
        for i in range(k):
            if max_val < P[i]:
                max_val = P[i]
                loc = i
        return loc

    def _copy_path(self, from_path, nfrom, to_path):
        """
        Copy path array.

        Parameters:
        from_path -- Source path array
        nfrom -- Number of elements to copy
        to_path -- Destination path array

        Returns:
        Number of elements copied
        """
        for i in range(nfrom):
            to_path[i] = from_path[i]
        return nfrom

    def UniformSampling(self, regime0, regime1, length, n1, n2, U):
        """
        Uniform sampling strategy.

        Parameters:
        current_regime -- Source SegBox
        regime0 -- First output SegBox
        regime1 -- Second output SegBox
        length -- Length of samples
        n1 -- First sample index
        n2 -- Second sample index
        U -- Uniform set SegBox
        """
        # Initialize segments
        regime0.reset()
        regime1.reset()

        i = int(n1 % U.num_segments)
        j = int(n2 % U.num_segments)

        st0 = U.segments[i]['start']
        st1 = U.segments[j]['start']

        # If overlapped, then ignore
        if abs(st0 - st1) < length:
            return

        regime0.add_segment(st0, length)
        regime1.add_segment(st1, length)

    def UniformSet(self, current_regime, length, trial, U):
        """
        Uniform set strategy.

        Parameters:
        current_regime -- Source SegBox
        length -- Length of samples
        trial -- Number of trials
        U -- Output SegBox for uniform set
        """
        slideW = int(math.ceil((current_regime.total_length - length) / trial))

        # Create uniform blocks
        U.reset()

        for i in range(current_regime.num_segments):
            if U.num_segments >= trial:
                return

            st = current_regime.segments[i]['start']
            ed = st + current_regime.segments[i]['duration']

            for j in range(trial):
                next_pos = st + j * slideW

                if next_pos + length > ed:
                    st = ed - length
                    if st < 0:
                        st = 0
                    U.add_segment_with_overlap(st, length)
                    break

                U.add_segment_with_overlap(next_pos, length)

    def FixedSampling(self, current_regime, regime0, regime1, length):
        """
        Fixed sampling strategy.

        Parameters:
        current_regime -- Source SegBox
        regime0 -- First output SegBox
        regime1 -- Second output SegBox
        length -- Length of samples
        """
        # Initialize segments
        regime0.reset()
        regime1.reset()

        # Segment regime0
        loc = 0 % current_regime.num_segments
        r = current_regime.segments[loc]['start']
        regime0.add_segment(r, length)

        # Segment regime1
        loc = 1 % current_regime.num_segments
        r = current_regime.segments[loc]['start'] + int(current_regime.segments[loc]['duration'] / 2)
        regime1.add_segment(r, length)

class HMM:
    """
    Hidden Markov Model class.

    Attributes:
        n: Number of sequences
        k: Number of states
        pi: Initial state probabilities
        A: Transition matrix
        pi_denom: Denominator for pi (for incremental estimation)
        A_denom: Denominator for A (for incremental estimation)
        d: Dimension of observations
        mean: Means for Gaussian emissions
        var: Variances for Gaussian emissions
        sum_w: Total weighted count
        M2: M2 for incremental variance computation
    """

    def __init__(self, variance):

        self.default_variance = variance

        self.n = 0
        self.k = 0
        self.pi = None
        self.A = None
        self.pi_denom = 0.0
        self.A_denom = None

        self.d = 0
        self.mean = None
        self.var = None
        self.sum_w = None
        self.M2 = None

    def _set_up(self, K, D):
        """Initialize an HMM with K states and D dimensions."""
        self.n = 0
        self.d = D
        self.k = K

        # Allocate arrays
        self.A = np.zeros((self.k, self.k), dtype=np.float64)
        self.pi = np.zeros(self.k, dtype=np.float64)
        self.mean = np.zeros((self.k, self.d), dtype=np.float64)
        self.var = np.zeros((self.k, self.d), dtype=np.float64)
        self.pi_denom = 0
        self.A_denom = np.zeros(self.k, dtype=np.float64)
        self.sum_w = np.zeros((self.k, self.d), dtype=np.float64)
        self.M2 = np.zeros((self.k, self.d), dtype=np.float64)

        # Random A matrix with EPSILON added, normalized row-wise
        self.A = np.random.rand(K, K) + EPSILON
        self.A /= self.A.sum(axis=1, keepdims=True)

        # Random pi with EPSILON added, normalized
        self.pi = np.random.rand(K) + EPSILON
        self.pi /= self.pi.sum()

        # Random means, fixed variance
        self.mean = np.random.rand(K, D) * MAX_NORMALISATION_VALUE
        self.var.fill(self.default_variance)

    def reset(self, K, D):
        """Reset an HMM to random values."""
        self.n = 0
        self.d = D
        self.k = K

        # Transition matrix A: random rows normalized
        self.A[:K, :K] = np.random.rand(K, K)
        self.A[:K, :K] /= self.A[:K, :K].sum(axis=1, keepdims=True)

        # Initial state probabilities pi: normalized
        self.pi[:K] = np.random.rand(K)
        self.pi[:K] /= self.pi[:K].sum()

        # Gaussian parameters: random means, fixed variance
        self.mean[:K, :D] = np.random.rand(K, D) * MAX_NORMALISATION_VALUE
        self.var[:K, :D] = self.default_variance

        # Reset accumulators
        self.pi_denom = 0.0
        self.A_denom[:K] = 0.0
        self.sum_w[:K, :D] = 0.0
        self.M2[:K, :D] = 0.0

    def copy_to(self, phmm2):
        """Copy HMM phmm1 to phmm2."""
        phmm2.d = self.d
        phmm2.k = self.k
        phmm2.n = self.n

        # Vectorized matrix/array copies
        phmm2.A[:self.k, :self.k] = self.A[:self.k, :self.k]
        phmm2.mean[:self.k, :self.d] = self.mean[:self.k, :self.d]
        phmm2.var[:self.k, :self.d] = self.var[:self.k, :self.d]
        phmm2.sum_w[:self.k, :self.d] = self.sum_w[:self.k, :self.d]
        phmm2.M2[:self.k, :self.d] = self.M2[:self.k, :self.d]
        phmm2.pi[:self.k] = self.pi[:self.k]
        phmm2.A_denom[:self.k] = self.A_denom[:self.k]

        # Scalar copy
        phmm2.pi_denom = self.pi_denom

class BAUM:
    """
    Baum-Welch algorithm data class.

    Attributes:
        alpha: Forward probabilities [mx][k]
        beta: Backward probabilities [mx][k]
        gamma: State probabilities [n][mx][k]
        gamma_space: Space for gamma calculations
        xi: Transition probabilities [n][mx][k][k]
        scale: Scaling factors [mx]
        idx: Cluster indices for k-means [n][mx]
        chmm: Current HMM model
    """

    def __init__(self, ap_seg):
        self.ap_seg = ap_seg

        self.alpha = None
        self.beta = None
        self.gamma = None
        self.gamma_space = None
        self.xi = None
        self.scale = None
        self.idx = None
        self.chmm = HMM(self.ap_seg.default_variance)

    def BaumWelch(self, phmm, n, xlst, wantKMEANS):
        """
        Perform the Baum-Welch algorithm for HMM training.

        Parameters:
        phmm -- HMM model to train
        n -- number of sequences
        xlst -- observation sequences
        baum -- BAUM data structure
        wantKMEANS -- flag for K-means initialization

        Returns:
        Lsum -- final log likelihood
        """

        l = 0
        alpha = self.alpha
        beta = self.beta
        gamma = self.gamma
        xi = self.xi
        scale = self.scale
        idx = self.idx

        if n == 0:
            raise ValueError('Estimation error (n == 0)')

        # If k==1, nothing to do (just run K-means)
        if phmm.k == 1:
            Kmeans(phmm, n, xlst, phmm.d, phmm.k, idx, self.ap_seg.max_k_means_iter, self.ap_seg)
            return -1

        if wantKMEANS:
            # If initial stage and want k-means
            if phmm.n == 0:
                phmm.reset(phmm.k, phmm.d)
                Kmeans(phmm, n, xlst, phmm.d, phmm.k, idx, self.ap_seg.max_k_means_iter, self.ap_seg)

        # Take absolute value of n
        n = int(abs(float(n)))

        prev_n = phmm.n
        phmm.n = n
        phmm.copy_to(self.chmm)

        # For each sequence
        Lsum = 0.0
        for r in range(phmm.n):
            O = xlst[r].O
            m = xlst[r].m

            # Cache emission probs for this sequence
            emission_probs = batch_pdf(phmm, phmm.k, m, O)  # shape: (K, m)

            Lf = self.forward(phmm, O, m, alpha, scale, emission_probs)
            self.backward(phmm, m, beta, scale, emission_probs)
            self._computeGamma(phmm, alpha, beta, gamma[r], m)
            self._computeXi(phmm, alpha, beta, xi[r], m, emission_probs)
            Lsum += Lf

        # Log likelihood
        Lpreb = Lsum

        # Baum-Welch iterations
        while True:
            # M-STEP: Update model parameters
            self.chmm.copy_to(phmm)
            self._computeParams(phmm, gamma, xi, xlst)

            # E-STEP: Re-estimate expectations
            Lsum = 0.0
            for r in range(phmm.n):
                O = xlst[r].O
                m = xlst[r].m

                # Cache emission probs for this sequence
                emission_probs = batch_pdf(phmm, phmm.k, m, O)  # shape: (K, m)

                Lf = self.forward(phmm, O, m, alpha, scale, emission_probs)
                self.backward(phmm, m, beta, scale, emission_probs)
                self._computeGamma(phmm, alpha, beta, gamma[r], m)
                self._computeXi(phmm, alpha, beta, xi[r], m, emission_probs)
                Lsum += Lf

            delta = Lpreb - Lsum
            Lpreb = Lsum
            l += 1

            # Check convergence
            if abs(delta) <= self.ap_seg.delta or l >= self.ap_seg.max_iter:
                break

        # For incremental fitting
        self._computeParams(phmm, gamma, xi, xlst)

        # Avoid numerical errors
        if math.isnan(Lsum):
            print("baumWelch: isnan, resetHMM...")
            phmm.reset(phmm.k, phmm.d)

        phmm.n = phmm.n + prev_n

        return Lsum

    def _computeParams(self, phmm, gamma, xi, xlst):
        """
        Compute HMM parameters based on gamma and xi values.

        Parameters:
        phmm -- HMM model to update
        gamma -- state probabilities
        xi -- transition probabilities
        xlst -- observation sequences
        """
        k = phmm.k
        # Initial probability vector pi
        # (a) recover previous pi*N
        phmm.pi[:k] *= phmm.pi_denom  # restore weighted prior

        # (b) add new gamma
        gamma0_sum = sum(g[0] for g in gamma[:k])  # g[0] is shape (K,)
        phmm.pi[:k] += EPSILON + gamma0_sum[:k]

        # # (c) normalize, clip negatives and normalize
        np.clip(phmm.pi[:k], 0, None, out=phmm.pi[:k])
        phmm.pi_denom = np.sum(phmm.pi[:k])

        phmm.pi[:k] /= phmm.pi_denom

        # Transition matrix A
        # (a) recover previous A*N
        phmm.A[:k, :k] *= phmm.A_denom[:k, None]  # (K, 1)

        # (b) add new xi
        # (b) accumulate all xi transitions
        A_update = np.zeros((k, k), dtype=np.float64)
        for r in range(phmm.n):
            m_r = xlst[r].m
            A_update += np.sum(xi[r][:m_r - 1], axis=0) + EPSILON  # sum over t

        phmm.A[:k, :k] += A_update

        np.clip(phmm.A, 0.0, None, out=phmm.A)
        phmm.A_denom = np.sum(phmm.A, axis=1)
        phmm.A /= phmm.A_denom[:, None]

        # Weighted incremental computation for means and variances
        for i in range(k):
            for j in range(phmm.d):
                mean = phmm.mean[i][j]
                M2 = phmm.M2[i][j]
                sum_w = phmm.sum_w[i][j]

                if sum_w == 0.0:
                    mean = 0.0
                    M2 = 0.0

                for r in range(phmm.n):
                    obs = xlst[r].O[:, j]  # shape: (T,)
                    w = gamma[r][:, i]  # shape: (T,)

                    for x, w_i in zip(obs, w):
                        tmp = w_i + sum_w + EPSILON
                        delta = x - mean
                        R = (delta * w_i) / tmp
                        mean += R
                        M2 += sum_w * delta * R
                        sum_w = tmp

                # Finalize variance
                var = M2 / sum_w if sum_w > 0 else 0.0
                var = min(self.ap_seg.max_variance, max(var, self.ap_seg.min_variance))

                phmm.mean[i][j] = mean
                phmm.M2[i][j] = M2
                phmm.sum_w[i][j] = sum_w
                phmm.var[i][j] = var

    def _set_up(self, n, m, k, d):
        """
        Allocate memory for Baum-Welch algorithm data structures.

        Parameters:
        baum -- BAUM data structure
        n -- number of sequences
        m -- max sequence length
        k -- number of states
        d -- dimension of observations
        """

        self.alpha = np.zeros((m, k), dtype=np.float64)  # dmatrix(0, m, 0, k)
        self.beta = np.zeros((m, k), dtype=np.float64)  # dmatrix(0, m, 0, k)
        self.scale = np.zeros(m, dtype=np.float64)  # dvector(0, m)
        self.idx = np.zeros((n, m), dtype=np.int32)  # imatrix(0, n, 0, m)

        # Gamma
        self.gamma = [None] * n
        for r in range(n):
            self.gamma[r] = np.zeros((m, k), dtype=np.float64)  # dmatrix(0, m, 0, k)

        # Xi
        self.xi = [None] * n
        for r in range(n):
            self.xi[r] = [None] * m
            for t in range(m):
                self.xi[r][t] = np.zeros((k, k), dtype=np.float64)  # dmatrix(0, k, 0, k)

        # For incremental EM
        self.chmm._set_up(k, d)

    # Function declarations from forbackward.c
    def forward(self, phmm, O, m, alpha, scale, emission_probs):
        """
        Run the Forward algorithm to calculate the likelihood of observations.

        Parameters:
        phmm -- HMM model
        O -- observation sequence
        m -- sequence length
        alpha -- forward probabilities matrix
        scale -- scaling factors array

        Returns:
        L -- log likelihood of the observation sequence
        """
        # Initialize scale
        scale[:m] = 0.0

        O0 = np.asarray([O[0]])  # shape: (1, D)
        obs_probs = batch_pdf(phmm, phmm.k, m, O0)[:, 0]  # shape: (K,)
        alpha[0, :phmm.k] = phmm.pi[:phmm.k] * obs_probs
        scale[0] = alpha[0].sum()

        alpha[0, :phmm.k] /= scale[0]

        # Induction
        for t in range(m - 1):
            # Matrix multiply: alpha[t] (1, K) x A (K, K) -> (K,)
            alpha_t1 = alpha[t, :phmm.k] @ phmm.A[:phmm.k, :phmm.k]  # shape: (K,)
            # Multiply by emission probabilities at time t+1
            alpha[t + 1, :phmm.k] = alpha_t1 * emission_probs[:phmm.k, t + 1]

            # Normalize and store scale
            scale[t + 1] = alpha[t + 1, :phmm.k].sum()
            alpha[t + 1, :phmm.k] /= scale[t + 1]

        # Termination
        return np.sum(np.log(scale[:m]))

    def backward(self, phmm, m, beta, scale, emission_probs):
        """
        Run the Backward algorithm.

        Parameters:
        phmm -- HMM model
        O -- observation sequence
        m -- sequence length
        beta -- backward probabilities matrix
        scale -- scaling factors array from Forward algorithm

        Returns:
        L -- log likelihood of the observation sequence
        """

        # Init (t=m-1)
        beta[m - 1, :phmm.k] = 1.0 / scale[m - 1]

        # Induction
        for t in range(m - 2, -1, -1):
            # Emission at time t+1 across all states (shape: K,)
            emis = emission_probs[:phmm.k, t + 1]

            # Elementwise multiply: emission * beta[t+1]
            weighted = emis * beta[t + 1, :phmm.k]  # shape: (K,)

            # Matrix multiply: A (KxK) x weighted (K,) → (K,)
            beta[t, :phmm.k] = phmm.A[:phmm.k, :phmm.k] @ weighted

            # Normalize by scale[t]
            beta[t] /= scale[t]

        # Log likelihood (same as Forward)
        return np.sum(np.log(scale[:m]))

    # Function declarations from baum.c
    def _computeGamma(self, phmm, alpha, beta, gamma, m):
        """
        Compute gamma values (state probabilities).

        Parameters:
        phmm -- HMM model
        alpha -- forward probabilities matrix
        beta -- backward probabilities matrix
        gamma -- output gamma matrix
        m -- sequence length
        """
        product = alpha[:m, :phmm.k] * beta[:m, :phmm.k]  # (m, K)
        gamma[:m, :phmm.k] = product / product.sum(axis=1, keepdims=True)

    def _computeXi(self, phmm, alpha, beta, xi, m, emission_probs):
        """
        Compute xi values (transition probabilities).

        Parameters:
        phmm -- HMM model
        O -- observation sequence
        alpha -- forward probabilities matrix
        beta -- backward probabilities matrix
        xi -- output xi tensor
        m -- sequence length
        """

        k = phmm.k
        A = phmm.A[:k, :k]  # shape: (K, K)

        for t in range(m - 1):
            # Shapes:
            # alpha[t]: (K,)
            # A: (K, K)
            # emission_probs[:, t+1]: (K,)
            # beta[t+1]: (K,)

            # Broadcast to shape (K, K)
            outer = alpha[t, :k][:, None] * A * emission_probs[:k, t + 1][None, :] * beta[t + 1, :k][None, :]

            # Normalize
            xi[t] = outer / np.sum(outer)

class VITERBI:
    """
    Viterbi algorithm data class.

    Attributes:
        delta: Viterbi probabilities [m][k]
        psi: Backtracking indices [m][k]
        q: State sequence [m]
        piL: Log initial probabilities [k]
        AL: Log transition probabilities [k][k]
        biot: Log emission probabilities [k][m]
    """

    def __init__(self, ap_seg):
        self.ap_seg = ap_seg

        self.delta = None
        self.psi = None
        self.q = None
        self.piL = None
        self.AL = None
        self.biot = None

    # Function declarations from viterbi.c
    def _set_up(self, n, m, k):
        """Allocate memory for Viterbi algorithm data structures."""
        self.delta = np.zeros((m, k), dtype=np.float64)
        self.psi = np.zeros((m, k), dtype=np.int32)
        self.q = np.zeros(m, dtype=np.int32)
        self.piL = np.zeros(k, dtype=np.float64)
        self.AL = np.zeros((k, k), dtype=np.float64)
        self.biot = np.zeros((k, m), dtype=np.float64)

    def _viterbi(self, phmm, delta, st, length):
        """
        Run Viterbi algorithm.

        Parameters:
        phmm -- HMM model
        delta -- Delta parameter
        st -- Start position
        length -- Segment length
        vit -- Viterbi data structure

        Returns:
        Coding cost
        """
        Lh = self.ViterbiL(phmm, length, self.ap_seg.x.O[st:st + length])

        if delta <= 0 or delta >= 1:
            raise ValueError('Delta must be 0.0 < delta < 1.0')

        Lh += math.log(delta)  # Switch
        Lh += (length - 1) * math.log(1.0 - delta)  # Else (stay)
        costC = -Lh / math.log(2.0)

        return costC

    def ViterbiL(self, phmm, m, O):
        """
        Run the Viterbi algorithm in log domain.

        Parameters:
        phmm -- HMM model
        m -- sequence length
        O -- observation sequence
        vit -- VITERBI data structure

        Returns:
        Lh -- log likelihood of the most probable path
        """

        k = phmm.k

        if m == 0:
            return 0

        self.TakelogHMM(phmm, m, O)

        delta = self.delta
        psi = self.psi
        q = self.q
        piL = self.piL
        AL = self.AL
        biot = self.biot

        # compute delta (t==0)
        delta[0, :k] = piL[:k] + biot[:k, 0]
        psi[0, :k] = 0

        # compute delta (t>0)
        for t in range(1, m):
            deltax = -VERY_LARGE_COST
            for j in range(phmm.k):
                maxval = -VERY_LARGE_COST
                maxvalind = 0
                for i in range(phmm.k):
                    val = delta[t - 1][i] + AL[i][j]
                    if val > maxval:
                        maxval = val
                        maxvalind = i
                delta[t][j] = maxval + biot[j][t]
                psi[t][j] = maxvalind
                if deltax < delta[t][j]:
                    deltax = delta[t][j]

        # final likelihood
        Lh = -VERY_LARGE_COST
        q[m - 1] = -1
        for i in range(phmm.k):
            if delta[m - 1][i] > Lh:
                Lh = delta[m - 1][i]
                q[m - 1] = i

        # avoid error
        if q[m - 1] == -1:
            raise ValueError("Cannot compute log Viterbi path")

        # check viterbi path
        for t in range(m - 2, -1, -1):
            q[t] = psi[t + 1][q[t + 1]]

        return Lh

    def TakelogHMM(self, phmm, m, O):
        """
        Take logarithm of HMM parameters for log-domain Viterbi algorithm.

        Parameters:
        phmm -- HMM model
        m -- sequence length
        O -- observation sequence
        """

        self.piL[:phmm.k] = np.log(phmm.pi[:phmm.k] + EPSILON)

        # vit.AL[:phmm.k, :phmm.k] = np.log(phmm.A[:phmm.k, phmm.k] + EPSILON)
        for i in range(phmm.k):
            for j in range(phmm.k):
                self.AL[i][j] = math.log(phmm.A[i][j] + EPSILON)

        m = min(m, O.shape[0])
        self.biot[:phmm.k, :m] = batch_log_pdf(phmm, phmm.k, m, O[:m])

# ------------------------------
#         KMEANS FUNCTIONS
# ------------------------------
def compute_cluster_stats(data, labels, k, d, ap_seg):
    """
    Compute per-cluster mean and variance (vectorized).
    Parameters:
        data: np.ndarray (N, D)
        labels: np.ndarray (N,)
        k: number of clusters
        d: dimensionality
    Returns:
        means: (k, D)
        vars: (k, D)
    """
    means = np.zeros((k, d))
    vars_ = np.zeros((k, d))

    for j in range(k):
        mask = labels == j
        cluster_points = data[mask]

        if len(cluster_points) == 0:
            # Fallback to random mean/var if empty
            means[j] = np.random.rand(d)
            vars_[j] = ap_seg.min_variance
            continue

        mean = np.mean(cluster_points, axis=0)
        var = np.var(cluster_points, axis=0, ddof=1)  # sample variance

        means[j] = mean
        vars_[j] = np.clip(var + EPSILON, ap_seg.min_variance, ap_seg.max_variance)

    return means, vars_

def Kmeans(phmm, n, xlst, d, k, idx, max_iter, ap_seg):
    """Perform K-means clustering.

    Parameters:
    phmm -- HMM model object with mean and var attributes
    n -- number of sequences
    xlst -- input data (list of Input objects with O and m attributes)
    d -- dimension of data
    k -- number of clusters
    idx -- cluster assignments (2D array[n][m])
    """
    # if k==1, nothing to do
    if k == 1:
        # Flatten all observations
        X = np.vstack([x.O for x in xlst[:n]])

        # Assign all to cluster 0
        for r in range(n):
            idx[r][:xlst[r].m] = 0

        # Compute global mean and variance
        phmm.mean[0] = np.mean(X, axis=0)
        var = np.var(X, axis=0, ddof=1)
        phmm.var[0] = np.clip(var + EPSILON, ap_seg.min_variance, ap_seg.max_variance)

        return

    # Flatten data
    X = np.vstack([x.O for x in xlst[:n]])  # shape: (N_total, D)

    kmeans = KMeans(n_clusters=k, init='random', n_init=1, max_iter=max_iter, random_state=42)

    labels = kmeans.fit_predict(X)

    offset = 0
    for r in range(n):
        for i in range(xlst[r].m):
            idx[r][i] = labels[offset]
            offset += 1

    # Final calculation of means and variances
    phmm.mean[:k], phmm.var[:k] = compute_cluster_stats(X, labels, k, d, ap_seg)

# ------------------------------
#         MATH FUNCTIONS
# ------------------------------
# MDL (Minimum Description Length) functions
def log_2(x):
    """Log base 2 of x."""
    return math.log(x) / math.log(2.0)

def log_s(x):
    """Compute 2*log_2(x) + 1."""
    return 2.0 * log_2(x) + 1.0

def batch_log_pdf(hmm, k, m, observations):
    observations = observations[:m]  # (T, D)
    mean = hmm.mean[:k] # (K, D)
    var = np.abs(hmm.var[:k])  # (K, D)

    # Expand dimensions for broadcasting: (K, T, D)
    observations_exp = observations[None, :, :]  # (1, T, D)
    mean_exp = mean[:, None, :]  # (K, 1, D)
    var_exp = var[:, None, :]  # (K, 1, D)

    # Gaussian PDF calculation
    diff_sq = (observations_exp - mean_exp) ** 2
    denom = np.sqrt(2 * np.pi * var_exp)
    exponent = -diff_sq / (2 * var_exp)

    p = np.exp(exponent) / denom

    # Clamp to [EPSILON, ONE_MINUS_EPSILON]
    p = np.clip(p, EPSILON, ONE_MINUS_EPSILON)

    # log(EPSILON + p)
    log_probs = np.log(EPSILON + p)

    # Sum over dimensions (D), result is (K, T)
    log_sums = log_probs.sum(axis=2)

    # Final clamp: if sum < log(EPSILON), set to log(EPSILON)
    return np.maximum(log_sums, LOG_EPSILON)

def batch_pdf(hmm, k, m, observations):
    return np.exp(batch_log_pdf(hmm, k, m, observations))
